{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"uva_seal.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib Clustering\n",
    "\n",
    "### University of Virginia\n",
    "### DS 5110: Big Data Systems\n",
    "### Last Updated: Mar 10, 2023\n",
    "\n",
    "---  \n",
    "\n",
    "\n",
    "### SOURCES  \n",
    "- Learning Spark, Chapter 11: Machine Learning with MLlib  \n",
    "\n",
    "- https://spark.apache.org/docs/latest/ml-clustering.html\n",
    "\n",
    "- [Silhouette score](https://en.wikipedia.org/wiki/Silhouette_(clustering))\n",
    "\n",
    "\n",
    "### OBJECTIVES\n",
    "Introduction to some of the major clustering techniques in MLlib using the DataFrame API\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Unsupervised learning\n",
    "- K-means\n",
    "- Silhouette Score\n",
    "- Mixture of Gaussians\n",
    "\n",
    "---\n",
    "\n",
    "**Unsupervised Learning**  \n",
    "In this task, labels are unknown and the analyst wishes to segment the observations into groups of high similarity, where similarity is defined in terms of the feature space.\n",
    "\n",
    "Common use cases are:\n",
    "- Data exploration to discover the properties of similar observations  \n",
    "- Outlier detection; outliers will generally form their own group (e.g., singletons)  \n",
    "\n",
    "**K-Means**  \n",
    "This is the most popular clustering algorithm, with widespread use in industry. It is relatively simple, uses a single parameter, and converges on a solution (but possibly not the global maximum).\n",
    "\n",
    "The following models are supported in `spark.mllib` with the DataFrame API:\n",
    "\n",
    "- K-means\n",
    "- Gaussian mixture\n",
    "- Power iteration clustering (PIC)\n",
    "- Latent Dirichlet allocation (LDA)\n",
    "- Bisecting k-means\n",
    "\n",
    "**<center>K-Means Specs</center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Item   | Description |\n",
    "| -------- | ----------- |\n",
    "| Supervised/Unsupervised | Unsupervised |\n",
    "| Initialization | Random Assignment |\n",
    "| Assumptions | Euclidean Distance |\n",
    "| Preprocessing | Scaling |\n",
    "| Parameters | $K:$ number of clusters |\n",
    "| Metrics | Inertia |\n",
    "| Strengths | One parameter, relatively simple |\n",
    "| Weaknesses | 1. May not find global optimum <br> 2. Can't handle non-quant data (e.g., categorical)<br> 3. Assumes spherical cluster shape|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Means Sample 2D Visualization ($K=3$)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"k_means_before_after.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| K-Means Sample Workflow | \n",
    "| -------- | \n",
    "| 1. feature selection | \n",
    "| 2. feature standardization | \n",
    "| 3. run algo for sequence of $K$ |  \n",
    "| 4. examine results and remediate outliers <br> <span style=\"color:red\">loop on 3-4 as needed</span>| \n",
    "| 5. select $K^*$, extract labels | \n",
    "| 6. enrich with domain knowledge | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Means: Selecting $K^*$**\n",
    "\n",
    "One method for selecting $K^*$ is by identifying the elbow in a scree plot.  At the inflection point, adding more clusters reduces WSS only marginally.  Generally, well-formed clusters are split apart, creating new ones.\n",
    "\n",
    "$$ Within Sum of Squares (WSS) = 1 - \\frac{Between Sum of Squares}{Total Sum of Squares} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='scree_plot_k_means2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Means Implementation**\n",
    "\n",
    "`MLlib` contains an implementation of `K-means` and also `kmeans||`  \n",
    "`kmeans||` provides a better initialization in parallel environments.  \n",
    "\n",
    "Included in the parameters,  \n",
    "initMode = 'random' or 'k-means||', where 'k-means||' is the default initialization method.\n",
    "\n",
    "\n",
    "**Methods:**  \n",
    "Can access `clusterCenters` as an array of vectors  \n",
    "Can call `predict()` on a new vector to return its assigned cluster;   this is the closest center.\n",
    "\n",
    "**K-Means Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Loads data\n",
    "df = spark.read.csv(\"kmeans_data.txt\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemble the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats =  ['f1','f2','f3']      \n",
    "assembler = VectorAssembler(inputCols=feats, outputCol=\"features\")\n",
    "dataset=assembler.transform(df)\n",
    "dataset.select(\"*\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the features in the first observation are saved in sparse format (since all values are zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a k-means model with k=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans().setK(2).setSeed(314).setMaxIter(10)\n",
    "model = kmeans.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Predictions\n",
    "\n",
    "note the transform() method does prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(dataset)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "print(\"Cluster Centers: \")\n",
    "centers = model.clusterCenters()\n",
    "print(centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice the cluster centers are very intuitive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Score\n",
    "\n",
    "The silhouette score measures the consistency within clusters.  \n",
    "\n",
    "It falls in range [-1, 1] where\n",
    "- a value close to 1 means that points are consistent\n",
    "- a value near 0 indicates overlapping clusters  \n",
    "- a negative value indicates observations assigned to incorrect clusters\n",
    "\n",
    "It is computed like this: \n",
    "\n",
    "1) For each observation (point), compute A and B with these definitions:\n",
    "\n",
    "   A : The mean distance of each point to its neighbors within its cluster (called the *mean intra-cluster distance*).\n",
    " \n",
    "   B : the mean distance of each point to its next-closest cluster (called the *mean nearest-cluster distance*)\n",
    "\n",
    "   For well-assigned points, quantity *A* should be much smaller than quantity *B*\n",
    "\n",
    "2) Compute for each point *i* the quantity:  $ s(i) = (B - A) / max(A, B)$\n",
    "\n",
    "3) The mean is then computed over all s(i) to arrive at the silhoutte score.\n",
    "\n",
    "You can read more about the metric [here](https://en.wikipedia.org/wiki/Silhouette_(clustering))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Model\n",
    "\n",
    "The *Gaussian Mixture Model* is a weighted combination of underlying Gaussian distributions, each with a fixed probability.  \n",
    "The *expectation-maximization algorithm* is used in `spark.mllib` to estimate the parameters.  \n",
    "There is a mean vector and a covariance matrix for each cluster.  \n",
    "\n",
    "**Fit Mixture of Two Gaussians**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import GaussianMixture\n",
    "\n",
    "# reuse data from K-Means example above\n",
    "\n",
    "gmm = GaussianMixture().setK(2).setSeed(314)\n",
    "model = gmm.fit(dataset)\n",
    "\n",
    "print(\"Gaussians shown as a DataFrame: \")\n",
    "print(\"component mean vectors\")\n",
    "model.gaussiansDF.select(\"mean\").show(truncate=False)\n",
    "\n",
    "print(\"component covariance matrices\")\n",
    "model.gaussiansDF.select(\"cov\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the mean vectors are very close to the k-means centroids.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY FOR YOURSELF (UNGRADED EXERCISES)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **K-Means: try different initialization mode**  \n",
    "i. Copy the k-means example in the cell below  \n",
    "ii. Change the setting and observe if the results change. Hint: use setInitMode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) **K-Means: Try different K**  \n",
    "i. Copy the k-means example in the cell below  \n",
    "ii. Rerun k-mean with different k and observe the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) We considered a scree plot above, where the within sum of squared errors is measured for various values of k.  \n",
    "Is it possible to reduce the within sum of squares to zero, and if so, how? and is this a good idea?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Answer: By setting k = n (the number of observations), each observation will be placed in its own cluster. This will drive the within sum of squares to zero. However, this doesn't provide useful information, as no clustering is taking place."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
