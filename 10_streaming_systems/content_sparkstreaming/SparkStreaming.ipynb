{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uva_seal.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Streaming\n",
    "\n",
    "### University of Virginia\n",
    "### DS 5110: Big Data Systems\n",
    "### Last Updated: April 16, 2023\n",
    "\n",
    "---  \n",
    "\n",
    "\n",
    "Sources:\n",
    "\n",
    "https://spark.apache.org/docs/2.2.0/streaming-programming-guide.html\n",
    "\n",
    "Learning Spark, Chapter 10: Spark Streaming\n",
    "\n",
    "### OBJECTIVES\n",
    "- Provide an overview of `Spark Streaming`\n",
    "- Discuss how `DStreams` work\n",
    "- Discuss the concept of window computations\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- StreamingContext\n",
    "- DStream\n",
    "- Window Operations\n",
    "- Receivers\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "*Spark Streaming* is a module for acting on data as soon as it arrives.   \n",
    "\n",
    "Enables scalable, high-throughput, fault-tolerant stream processing of live data streams.\n",
    "\n",
    "Data can be ingested from many sources like `Kafka`, `Flume`, `Kinesis`, or `TCP sockets`, and can be processed using complex algorithms expressed with high-level functions like `map`, `reduce`, `join` and `window.`  \n",
    "\n",
    "Finally, processed data can be pushed out to filesystems, databases, and live dashboards.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"streaming-arch.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provides high-level abstraction called `discretized stream` or `DStream`, which represents continuous stream of data.  \n",
    "\n",
    "Internally, a `DStream` is a sequence of `RDDs`.  The time interval covered by each `RDD` is the same, and is called the `batch interval`.  \n",
    "\n",
    "`DStreams` are created from input data streams like Kafka, or by applying operations on other `DStreams`\n",
    "\n",
    "**Example: Word Count of Data Received From Server Listening on TCP Socket**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**NOTE: CODE IS FOR ILLUSTRATION PURPOSES ONLY**\n",
    "```\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working threads\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[2]\")\\\n",
    "        .appName(\"NetworkWordCount\")\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# set up StreamingContext with batch interval of 1 second\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Create a DStream that will connect to socket = hostname:port, like localhost:8888\n",
    "# on Windows machine, can find port from CMD by issuing: netstat -a -o\n",
    "lines = ssc.socketTextStream(\"localhost\", 8888)\n",
    "\n",
    "type(lines)\n",
    "# <class 'pyspark.streaming.dstream.DStream'>\n",
    "\n",
    "# Split each line into words\n",
    "# Each line is split into words, and this stream of words goes to DStream\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# MapReduce to count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print first ten elements of each RDD generated in DStream to console\n",
    "wordCounts.pprint()\n",
    "\n",
    "# Start the computation\n",
    "# depending where you are listening, different results will follow\n",
    "ssc.start()\n",
    "\n",
    "# Wait for the computation to terminate\n",
    "ssc.awaitTermination()  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red'> NOTE: </span> In practice, when running on a cluster, `master` won't be hardcoded in program.  \n",
    "\n",
    "Instead, launch application with `spark-submit` and receive it there. For local testing and unit tests, `local[*]` makes sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that no computation takes place until `start()` is called on `StreamingContext`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Launching the Application**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the example, you would first install a utility called `Netcat`, add it to your `PATH` variable, and issue the following at terminal.  Below the `nc` command, you can type text which should print in the Spark job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TERMINAL 1: Running Netcat on port 9999**\n",
    "```\n",
    "$ nc -lk 9999\n",
    "\n",
    "hello world\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a second terminal, you can run the program (provided as a Spark code example).  It should print out once per second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TERMINAL 2: Running the Spark job**\n",
    "```\n",
    "$ ./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results should look something like this:\n",
    "\n",
    "```\n",
    "-------------------------------------------\n",
    "Time: 2014-10-14 15:25:21\n",
    "-------------------------------------------\n",
    "(hello,1)\n",
    "(world,1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DStreams\n",
    "\n",
    "The `DStream` is the basic abstraction for Spark Streaming.  Each `RDD` in a `DStream` covers a time interval.  Any operation on a `DStream` maps onto each underlying RDD.\n",
    "\n",
    "The `RDD` transformations are computed by the Spark engine.  \n",
    "\n",
    "**DStream as a Sequence of RDDs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='streaming-dstream.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receivers\n",
    "\n",
    "For each input source, `Spark Streaming` launches *receivers*, which are tasks running on each executor that collect data from the input source and saves to `RDDs`. \n",
    "\n",
    "The receivers replicate the data to another executor for fault tolerance.  It is stored in executor memory, as in cached `RDDs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='exec_spark_streaming.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations on DStreams\n",
    "\n",
    "`DStreams` support many of the transformations available to `RDDs`.   Refer to documentation for details. https://spark.apache.org/docs/2.2.0/streaming-programming-guide.html\n",
    "\n",
    "Transformations on `DStreams` can be characterized as *stateless* or *stateful*.\n",
    "\n",
    "- In stateless transformations, batches can be treated independently.  Previous batch data is not required.  Examples include `map` and `filter`.  \n",
    "\n",
    "\n",
    "- In stateful transformations, data from previous batches is needed to compute results on the current batch.  Examples include window operations and `updateStateByKey()`, which can build up an object representing each user session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Operations\n",
    "\n",
    "`Windowed computations` allow for application of transformations over a sliding window of data.  In the example below, `window length=3` and `sliding interval=2`.  This means the operation is applied over the previous 3 `RDDs`, and it slides by 2 time units.\n",
    "\n",
    "<span style='color:red'>NOTE: </span> the parameters must be multiples of the batch interval of the `DStream`.\n",
    "\n",
    "**Windowed Computation Schematic**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='streaming-dstream-window.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wish to compute word counts for last 30 seconds of data, every 10 seconds, we can use `reduceByKey` on the `DStream` of `(word, 1)` pairs from the `Map` stage.\n",
    "\n",
    "**Compute Word Count over Last 30sec Data, every 10sec**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "```\n",
    "windowedWordCounts = pairs.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 30, 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several window operations.  For details, refer to: https://spark.apache.org/docs/2.2.0/streaming-programming-guide.html\n",
    "\n",
    "- `window()`\n",
    "- `countByWindow()`\n",
    "- `reduceByWindow()`\n",
    "- `reduceByKeyAndWindow()`\n",
    "- `countByValueAndWindow()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLLib Operations\n",
    "\n",
    "`SparkStreaming` is compatable with `MLlib`\n",
    "\n",
    "`MLlib` includes streaming ML algorithms which can learn from streaming data while doing scoring.\n",
    "\n",
    "Can alternatively learn a model offline and score the streaming data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching\n",
    "\n",
    "Calling `persist()` on `DStream` will persist all underlying `RDDs`.  \n",
    "This is useful when computing on the same data multiple times.  \n",
    "For window operations, this always occurs, thus the `DStreams` are automatically persisted in memory.\n",
    "\n",
    "For input streams that receive data over the network (e.g., Kafka), persisting the data will replicate on two nodes (by default) for fault-tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpointing\n",
    "\n",
    "Streaming applications must be resilient to failures external to the application (e.g., power outage, system maintenance).\n",
    "\n",
    "Information is `checkpointed` to a fault-tolerant storage system so it can easily recover from failures (e.g., `HDFS`, `S3`).  Checkpointing will save regular backups, such that the maximum information loss is equal to the information generated since the last checkpoint.\n",
    "\n",
    "Two types of information are checkpointed:\n",
    "- metadata    \n",
    "  - this backs up data on the driver node, including configs\n",
    "- data        \n",
    "  - generated RDDs   \n",
    "  - especially important if stateful transformations are used.\n",
    "  \n",
    "  setting up checkpointing is a one-liner:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "ssc.checkpoint(checkpointDirectory)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Operations\n",
    "\n",
    "Output operations bring the stream data to rest (e.g., store in a table in a database, print to screen).  They do the computation on each batch.\n",
    "\n",
    "Like actions on `RDDs`, `DStreams` are lazily evaluated.  \n",
    "When an output operation is called on a `DStream`, the computation takes place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming UI\n",
    "\n",
    "The Spark UI (typically located at *http://[driver]:4040*) contains a Streaming tab.  Here is a snapshot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='webui-streaming1.png'>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
