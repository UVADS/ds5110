{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uva_seal.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Feature Utilities\n",
    "\n",
    "### University of Virginia\n",
    "### DS 5110: Big Data Systems\n",
    "### Last Updated: March 23, 2021\n",
    "\n",
    "---  \n",
    "\n",
    "### SOURCES\n",
    "- Learning Spark, Chapter 11: Machine Learning with MLlib  \n",
    "- https://spark.apache.org/docs/latest/ml-features.html\n",
    "\n",
    "### OBJECTIVES\n",
    "- Discuss functions for extracting features from raw data\n",
    "- Discuss functions for transforming features\n",
    "- Discuss functions for selecting features\n",
    "\n",
    "### CONCEPTS AND FUNCTIONS\n",
    "\n",
    "The MLlib documentation contains a long list of functions in this area. Below we list some of the more important ones:\n",
    "\n",
    "- Feature Extractors\n",
    "    - TF-IDF\n",
    "    - Word2Vec \n",
    "    - CountVectorizer  \n",
    "    \n",
    "- Feature Transformers\n",
    "    - Tokenizer\n",
    "    - StopWordsRemover\n",
    "    - n-gram\n",
    "    - Binarizer\n",
    "    - OneHotEncoder\n",
    "    - Normalizer\n",
    "    - StandardScaler\n",
    "    - MaxAbsScaler\n",
    "    - Bucketizer\n",
    "    - VectorAssembler\n",
    "    - Imputer \n",
    "    \n",
    "- Feature Selectors\n",
    "    - ChiSqSelector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer  \n",
    "\n",
    "Break text into individual terms (e.g., words). In some languages this is a non-trivial task.  \n",
    "\n",
    "**Tokenizer Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|\n",
      "|  1|I wish Java could...|[i, wish, java, c...|\n",
      "|  2|Logistic regressi...|[logistic, regres...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load pyspark modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "spark= SparkSession.builder.getOrCreate()\n",
    "\n",
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I wish Java could use case classes\"),\n",
    "    (2, \"Logistic regression models are neat\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "\n",
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StopWordsRemover\n",
    "\n",
    "In text processing, stop words are words considered uninformative for analysis.  One of the first preprocessing steps is to remove stop words.  \n",
    "`StopWordsRemover` takes a sequence of strings and drops all stop words.  Optionally, the function takes a list of stop words.  Default lists are provided for some languages.\n",
    "\n",
    "**StopWordsRemover Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------+--------------------+\n",
      "|id |raw                         |filtered            |\n",
      "+---+----------------------------+--------------------+\n",
      "|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n",
      "|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n",
      "+---+----------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load pyspark modules\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
    "    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
    "], [\"id\", \"raw\"])\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
    "removedData = remover.transform(sentenceData)\n",
    "removedData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram\n",
    "An *n-gram* is a sequence of $n$ adjacent tokens (generally words).\n",
    "These objects can be useful as features in a model (e.g., indicators of presence of an *n-gram* in texts).\n",
    "\n",
    "**n-gram Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|               words|              ngrams|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|[Hi, I, heard, ab...|[Hi I, I heard, h...|\n",
      "|  1|[I, wish, Java, c...|[I wish, wish Jav...|\n",
      "|  2|[Logistic, regres...|[Logistic regress...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load pyspark modules\n",
    "from pyspark.ml.feature import NGram\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "wordDataFrame = spark.createDataFrame([\n",
    "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n",
    "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n",
    "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "\n",
    "ngramDataFrame = ngram.transform(wordDataFrame)\n",
    "ngramDataFrame.show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarizer\n",
    "\n",
    "Threshold variable to binary (0/1) features.\n",
    "Useful for presence/absence, for example.\n",
    "Feature values greater than a given threshold are binarized to 1.0; values equal to or less than the threshold are binarized to 0.0. Both Vector and Double types are supported for inputCol.\n",
    "\n",
    "**Binarizer Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|feature|\n",
      "+---+-------+\n",
      "|  0|    0.1|\n",
      "|  1|    0.8|\n",
      "|  2|    0.2|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load pyspark modules\n",
    "from pyspark.ml.feature import Binarizer\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "continuousDataFrame = spark.createDataFrame([\n",
    "    (0, 0.1),\n",
    "    (1, 0.8),\n",
    "    (2, 0.2)\n",
    "], [\"id\", \"feature\"])\n",
    "\n",
    "continuousDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarizer output with Threshold = 0.500000\n",
      "+---+-------+-----------------+\n",
      "| id|feature|binarized_feature|\n",
      "+---+-------+-----------------+\n",
      "|  0|    0.1|              0.0|\n",
      "|  1|    0.8|              1.0|\n",
      "|  2|    0.2|              0.0|\n",
      "+---+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binarizer = Binarizer(threshold=0.5, inputCol=\"feature\", outputCol=\"binarized_feature\")\n",
    "\n",
    "binarizedDataFrame = binarizer.transform(continuousDataFrame)\n",
    "\n",
    "print(\"Binarizer output with Threshold = %f\" % binarizer.getThreshold())\n",
    "binarizedDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+\n",
      "| id|feature1|feature2|\n",
      "+---+--------+--------+\n",
      "|  0|     0.1|    0.62|\n",
      "|  1|     0.8|    0.51|\n",
      "|  2|     0.2|     0.0|\n",
      "+---+--------+--------+\n",
      "\n",
      "+---+--------+--------+-------+-------+\n",
      "| id|feature1|feature2|output1|output2|\n",
      "+---+--------+--------+-------+-------+\n",
      "|  0|     0.1|    0.62|    0.0|    1.0|\n",
      "|  1|     0.8|    0.51|    1.0|    0.0|\n",
      "|  2|     0.2|     0.0|    0.0|    0.0|\n",
      "+---+--------+--------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# multiple column case added in Spark 3.0\n",
    "\n",
    "continuousDataFrame = spark.createDataFrame([\n",
    "    (0, 0.1, 0.62),\n",
    "    (1, 0.8, 0.51),\n",
    "    (2, 0.2, 0.0)\n",
    "], [\"id\", \"feature1\", \"feature2\"])\n",
    "\n",
    "continuousDataFrame.show()\n",
    "binarizer = Binarizer(thresholds=[0.5, 0.61])\n",
    "binarizer.setInputCols([\"feature1\", \"feature2\"]).setOutputCols([\"output1\", \"output2\"])\n",
    "binarizer.transform(continuousDataFrame).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHotEncoder\n",
    "\n",
    "Maps a column of label indices to a column of binary vectors, with at most a single one-value. \n",
    "This is the same as dummy coding.\n",
    "This encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features.\n",
    "\n",
    "An intermediate step is to use `StringIndexer`. \n",
    "\n",
    "`StringIndexer` encodes a string column of labels to a column of label indices. The indices are in [0, numLabels), ordered by label frequencies, so the most frequent label gets index 0.\n",
    "\n",
    "**OneHotEncoder Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pyspark modules\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "spark= SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"a\"),\n",
    "    (1, \"b\"),\n",
    "    (2, \"c\"),\n",
    "    (3, \"a\"),\n",
    "    (4, \"a\"),\n",
    "    (5, \"c\")\n",
    "], [\"id\", \"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for each level, count freq. val=0 for most freq, then 1, ...\n",
    "stringIndexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+-------------+\n",
      "| id|category|categoryIndex|  categoryVec|\n",
      "+---+--------+-------------+-------------+\n",
      "|  0|       a|          0.0|(2,[0],[1.0])|\n",
      "|  1|       b|          2.0|    (2,[],[])|\n",
      "|  2|       c|          1.0|(2,[1],[1.0])|\n",
      "|  3|       a|          0.0|(2,[0],[1.0])|\n",
      "|  4|       a|          0.0|(2,[0],[1.0])|\n",
      "|  5|       c|          1.0|(2,[1],[1.0])|\n",
      "+---+--------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\n",
    "model = encoder.fit(indexed)\n",
    "encoded = model.transform(indexed)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizer\n",
    "\n",
    "Transform each feature vector to have unit norm.  The type of norm (*p-norm*) is passed as a parameter, with $p=2$ the default (Euclidean distance).  Standardizing the features puts them on the same scale, which can be critical for models where scale matters (e.g., k-nearest neighbor).\n",
    "\n",
    "**Normalizer Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.5, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.0, 1.0]),),\n",
    "    (2, Vectors.dense([4.0, 10.0, 2.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "# Normalize each Vector using $L^1$ norm.\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "l1NormData = normalizer.transform(dataFrame)\n",
    "print(\"Normalized using L^1 norm\")\n",
    "l1NormData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler\n",
    "\n",
    "Transforms each feature vector to have mean zero (centering), standard deviation one (scaling).\n",
    "\n",
    "Parameters\n",
    "\n",
    "- withStd: True by default. Scales the data to unit standard deviation.\n",
    "- withMean: False by default. Centers the data with mean before scaling. \n",
    "\n",
    "Important Note\n",
    "If the data is sparse (mostly zeros), centering will make it dense, destroying the sparsity.   \n",
    "For cases of sparsity, other scaling techniques are preferable, such as MaxAbsScaler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxAbsScaler\n",
    "\n",
    "Transforms each feature vector to take values in the range [-1, 1] by dividing through by the maximum absolute value in each feature. It does not center the data, and thus does not destroy sparsity.\n",
    "\n",
    "**MaxAbsScaler Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------------+\n",
      "|features      |scaledFeatures                  |\n",
      "+--------------+--------------------------------+\n",
      "|[1.0,0.1,-8.0]|[0.25,0.010000000000000002,-1.0]|\n",
      "|[2.0,1.0,-4.0]|[0.5,0.1,-0.5]                  |\n",
      "|[4.0,10.0,6.0]|[1.0,1.0,0.75]                  |\n",
      "+--------------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -8.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.0, -4.0]),),\n",
    "    (2, Vectors.dense([4.0, 10.0, 6.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "scaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Compute summary statistics and generate MaxAbsScalerModel\n",
    "scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "# rescale each feature to range [-1, 1].\n",
    "scaledData = scalerModel.transform(dataFrame)\n",
    "\n",
    "scaledData.select(\"features\", \"scaledFeatures\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketizer\n",
    "\n",
    "Transform a column of continuous features to a column of feature buckets. \n",
    "\n",
    "Parameter\n",
    "\n",
    "splits: with n+1 splits, there are n buckets. A bucket defined by splits x,y holds values in the range [x,y) except the last bucket, which also includes y. Splits should be strictly increasing. Values at -inf, inf must be explicitly provided to cover all Double values; Otherwise, values outside the splits specified will be treated as errors. \n",
    "\n",
    "Two examples of splits are Array(Double.NegativeInfinity, 0.0, 1.0, Double.PositiveInfinity) and Array(0.0, 1.0, 2.0).\n",
    "\n",
    "**Bucketizer Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "splits = [-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")]\n",
    "\n",
    "data = [(-999.9,), (-0.5,), (-0.3,), (0.0,), (0.2,), (999.9,)]\n",
    "dataFrame = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"features\", outputCol=\"bucketedFeatures\")\n",
    "\n",
    "# Transform original data into its bucket index.\n",
    "bucketedData = bucketizer.transform(dataFrame)\n",
    "\n",
    "print(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits())-1))\n",
    "bucketedData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorAssembler\n",
    "\n",
    "`VectorAssembler` is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees.  \n",
    "\n",
    "VectorAssembler accepts the following input column types: \n",
    "\n",
    "- numeric\n",
    "- boolean\n",
    "- vector\n",
    "\n",
    "In each row, the values of the input columns will be concatenated into a vector in the specified order.\n",
    "\n",
    "**VectorAssembler Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\n",
      "+---+----+------+--------------+-------+-----------------------+\n",
      "|id |hour|mobile|userFeatures  |clicked|features               |\n",
      "+---+----+------+--------------+-------+-----------------------+\n",
      "|0  |18  |1.0   |[0.0,10.0,0.5]|1.0    |[18.0,1.0,0.0,10.0,0.5]|\n",
      "+---+----+------+--------------+-------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "dataset = spark.createDataFrame(\n",
    "    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],\n",
    "    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"hour\", \"mobile\", \"userFeatures\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(dataset)\n",
    "print(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\n",
    "output.select(\"*\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputer\n",
    "\n",
    "Imputes missing values using mean (the default) or median in columns where missing values are located.  The inputs columns should be of DoubleType or FloatType. \n",
    "\n",
    "Important Note  \n",
    "Currently `Imputer` does not support categorical features and possibly creates incorrect values for columns containing \n",
    "categorical features.\n",
    "\n",
    "**Imputer Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (1.0, float(\"nan\")),\n",
    "    (2.0, float(\"nan\")),\n",
    "    (float(\"nan\"), 3.0),\n",
    "    (4.0, 4.0),\n",
    "    (5.0, 5.0)\n",
    "], [\"a\", \"b\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(inputCols=[\"a\", \"b\"], outputCols=[\"out_a\", \"out_b\"])\n",
    "model = imputer.fit(df)\n",
    "\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChiSqSelector\n",
    "\n",
    "Oftentimes there is a large set of potential features and we need a way to select a “good” subset. \n",
    "\n",
    "`ChiSqSelector` uses the Chi-Squared test of independence for feature selection. It operates on labeled data with categorical features.  \n",
    "\n",
    "It supports five selection methods: numTopFeatures, percentile, fpr, fdr, fwe: * \n",
    "\n",
    "*numTopFeatures* chooses a fixed number of top features according to a chi-squared test. This is akin to yielding the features with the most predictive power.  \n",
    "\n",
    "See documentation for details\n",
    "\n",
    "**ChiSqSelector Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pyspark modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "spark= SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (7, Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0,),\n",
    "    (8, Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.0,),\n",
    "    (9, Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.0,)], [\"id\", \"features\", \"clicked\"])\n",
    "\n",
    "selector = ChiSqSelector(numTopFeatures=2, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"clicked\")\n",
    "\n",
    "result = selector.fit(df).transform(df)\n",
    "\n",
    "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY FOR YOURSELF (UNGRADED EXERCISES)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **N-Gram Generation**  \n",
    "i. Create your own DataFrame containing sentences.  \n",
    "ii. Chain the following steps together: `Tokenizer`, `StopWordsRemover`, `NGram` for one or more ngram choices (bigrams, trigrams, ...).  \n",
    "iii. Show the DataFrame containing containing the n-grams you've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) **Vector Assembler**  \n",
    "i. Create a DataFrame with a column of quantitative data, and a column of text data (such as sentences).  \n",
    "ii. For each data type, perform some transformations.  For example, you can run `StandardScaler` on the quantitative data.  This produces two features.   \n",
    "iii. Use `VectorAssember` to bind the two features into a single feature.  \n",
    "iv. Show the DataFrame containing the assembled vector.  For downstream modeling, this vector would $X$ in the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
