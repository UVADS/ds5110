### Reading: Distributed Model Training

---

Note: Some of these readings are relatively short

[Distributed Machine Learning Frameworks and its Benefits](https://www.xenonstack.com/blog/distributed-ml-framework#:~:text=In%20distributed%20machine%20learning%2C%20model,and%20training%20each%20split%20separately.)

[Distributed Training: Guide for Data Scientists](https://neptune.ai/blog/distributed-training)

[Distributed Machine Learning and the Parameter Server](https://www.cs.cornell.edu/courses/cs4787/2019sp/notes/lecture22.pdf)

Google AI developed a software framework called `DistBelief` that can utilize computing clusters with thousands of machines to train large models.
[Large Scale Distributed Deep Networks](https://static.googleusercontent.com/media/research.google.com/en//archive/large_deep_networks_nips2012.pdf)

Uber Engineering introduced Horovod, an open-source component of Michelangeloâ€™s deep learning toolkit which makes it easier to start and speed up distributed deep learning with TensorFlow. [Horovod: fast and easy distributed deep learning in TensorFlow](https://arxiv.org/pdf/1802.05799)

Foundational paper on Federated Learning, which allows for global model development on distributed data that can't be moved from locally-partitioned data. [Communication-Efficient Learning of Deep Networks from Decentralized Data](https://arxiv.org/abs/1602.05629)
