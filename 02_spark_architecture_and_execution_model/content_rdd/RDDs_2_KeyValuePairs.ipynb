{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uva_seal.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Value Pairs\n",
    "\n",
    "### University of Virginia\n",
    "### DS 5110: Big Data Systems\n",
    "### Last Updated: February 1, 2023\n",
    "\n",
    "---  \n",
    "\n",
    "### SOURCES \n",
    "\n",
    "1. Learning Spark 1st Ed., Chapter 4: Working with Key/Value Pairs\n",
    "\n",
    "### OBJECTIVES\n",
    "1. Learn about properties and methods for pair RDDs\n",
    "\n",
    "\n",
    "### CONCEPTS AND FUNCTIONS\n",
    "- Pair RDDs  \n",
    "- Partition  \n",
    "- `reduceByKey()`, `groupByKey()`, `combineByKey()`, `sortByKey()`  \n",
    "- `mapValues()`, `flatMapValues()`  \n",
    "- `keys()`, `values()`  \n",
    "- `join()`, `subtractByKey()`, `rightOuterJoin()`, `leftOuterJoin()`, `cogroup()`  \n",
    "- `countByKey()`  \n",
    "- `collectAsMap()`  \n",
    "- `lookup()`  \n",
    "- `groupWith()`  \n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAIR RDD BASICS\n",
    "\n",
    "A *Pair RDD* contains key/value pairs (e.g., dictionary in Python).  \n",
    "Oftentimes data scientists will talk about the *key* of a record, meaning the field on which it will be aggregated.  For example, if the records contain employee titles, and we wish to compute salary statistics by title, then it makes sense for the key to be title, and the values are salaries.\n",
    "\n",
    "In broader terms, Pair RDDs are useful for merging and aggregating data.  \n",
    "\n",
    "Applying the `map()` function against an RDD will produce a Pair RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"pair_rdd\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.parallelize(['french fries','chicken burrito','Apache Spark', 'OpenAI ChatGPT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = lines.map(lambda x: (x.split(\" \")[0], x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Transformations\n",
    "\n",
    "Some examples:\n",
    "\n",
    "`reduceByKey()`  \n",
    "Runs several parallel reduce operations, one for each key.  \n",
    "Combining is done locally on each machine for each key before computing a global combine for the key.  \n",
    "This reduces shuffling of the data across nodes, which is expensive.\n",
    "\n",
    "`fold()`  \n",
    "Similar to `reduce()`, `fold()` includes a “zero value” or starting point, which acts as the identity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD\n",
    "rdd = sc.parallelize([(1,2),(3,4),(3,6),(-1,10),(-1,22)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the keys\n",
    "rdd.keys().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTICE: Does the `keys()` function dedupe the keys (remove duplicates)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce (sum) by keys\n",
    "rdd.reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY FOR YOURSELF (UNGRADED EXERCISE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below uses a different operator in the `reducer`.  What do you expect the output to look like?  Now run it to verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.reduceByKey(lambda x,y: x*y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Revisiting Word Count**\n",
    "\n",
    "Now that you have an understanding of Pair RDDs, review the word count program below to see how they are used.  The `map()` creates the Pair RDDs, and `reduceByKey()` is a `reducer` operating on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"README.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts = lines.map(lambda x: x.replace(',',' ') \\\n",
    "                        .replace('.','   ').replace('-',' ').lower()) \\\n",
    "                        .flatMap(lambda x: x.split()) \\\n",
    "                        .map(lambda x: (x, 1)) \\\n",
    "                        .reduceByKey(lambda x,y:x+y) \\\n",
    "                        .map(lambda x:(x[1],x[0])) \\\n",
    "                        .sortByKey(False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY FOR YOURSELF (UNGRADED EXERCISE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`wordcounts` contains $(count, word)$ pairs for the top 10 most frequent words.    Write code to save only the words in a list.  Test that it works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding Frequent Word Bigrams**\n",
    "\n",
    "A frequent task in natural language processing (NLP) is generating bigrams from text, which are pairs of adjacent words.\n",
    "\n",
    "Let's demonstrate bigrams by parsing this sentence above into its bigrams.  The code below will apply a `mapper` to generate $(word, 1)$ key value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = sc.parallelize(['A frequent task in natural language processing (NLP) is generating bigrams from text, which are pairs of adjacent words.'])\n",
    "\n",
    "bigrams = text \\\n",
    "            .map(lambda x: x.split()) \\\n",
    "            .flatMap(lambda x: [((x[i],x[i+1]),1) for i in range(0,len(x)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output from the bigram code\n",
    "\n",
    "[(('A', 'frequent'), 1), \n",
    " (('frequent', 'task'), 1), \n",
    " (('task', 'in'), 1), \n",
    " (('in', 'natural'), 1), \n",
    " (('natural', 'language'), 1), \n",
    " (('language', 'processing'), 1), \n",
    " (('processing', '(NLP)'), 1), (('(NLP)', 'is'), 1), \n",
    " (('is', 'generating'), 1), (('generating', 'bigrams'), 1), \n",
    " (('bigrams', 'from'), 1), \n",
    " (('from', 'text,'), 1), \n",
    " (('text,', 'which'), 1), \n",
    " (('which', 'are'), 1), \n",
    " (('are', 'pairs'), 1), \n",
    " (('pairs', 'of'), 1), \n",
    " (('of', 'adjacent'), 1), \n",
    " (('adjacent', 'words.'), 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we modify the code snippet to do a word count on the bigrams.\n",
    "Specifically, we include a `reduceByKey()` and `sortByKey()` on the PairRDDs to count the frequency of each bigram.  This won't be super fascinating, as each bigram appears only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram Word Count\n",
    "\n",
    "bigrams = text \\\n",
    "          .map(lambda x: x.split()) \\\n",
    "          .flatMap(lambda x: [((x[i],x[i+1]),1) for i in range(0,len(x)-1)])\\\n",
    "          .reduceByKey(lambda x,y: x+y) \\\n",
    "          .map(lambda x: (x[1],x[0])) \\\n",
    "          .sortByKey(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY FOR YOURSELF (UNGRADED EXERCISE)**\n",
    "\n",
    "Let's make the bigram word count a little more interesting, shall we?  \n",
    "\n",
    "The cell below defines `text_two` to be the same as `text`.  \n",
    "\n",
    "Modify the text by inserting some duplicate bigrams.  \n",
    "\n",
    "Next copy the Bigram Word Count code below, running it on `text_two`.  \n",
    "\n",
    "Notice the difference in output.  Does it make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_two = sc.parallelize(['A frequent task in natural language processing (NLP) is generating bigrams from text, which are pairs of adjacent words.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy Bigram Word Count code here, running on text_two\n",
    "\n",
    "# collect the bigrams to review results. for a massive dataset,\n",
    "# collecting all the bigrams could crash your driver (out of memory exception)\n",
    "\n",
    "bigrams.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Partition**  \n",
    "\n",
    "Partitions determine the amount of parallelism when executing on RDDs.  \n",
    "Most operators in this chapter take a parameter for partitioning.  \n",
    "\n",
    "Example: reduceByKey(lambda x, y: x + y, 10)\n",
    "\n",
    "**Join**  \n",
    "We can build up a dataset by joining it with other datasets on one or more keys.  Here are some common join operations:  \n",
    "\n",
    "`join()`  is an inner join   \n",
    "`leftOuterJoin()`    # keep all records from the left table, match on right table  \n",
    "`rightOuterJoin()`   # keep all records from the right table, match on left table\n",
    "\n",
    "**Sorting**  \n",
    "Sort functions take a parameter for sort direction.  \n",
    "It is possible to provide a comparison function for custom sorting.  \n",
    "\n",
    "Here is an example of converting integers to strings and using a string compare function:  \n",
    "\n",
    "```\n",
    "rdd.sortByKey(ascending=True, numPartitions=None, keyfunc = lambda x: str(x))  \n",
    "```\n",
    "\n",
    "**Actions on Pair RDDs**  \n",
    "All transformations for RDDs are avail for Pair RDDs, plus additional ones like:  \n",
    "`countByKey()`  \n",
    "`collectAsMap()`  \n",
    "`lookup()`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a Pair RDD\n",
    "rdd = sc.parallelize([(1,2),(3,4),(3,6),(5,1),(5,10),(5,100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a dictionary where values are counts on each key\n",
    "rdd.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a list of values for key=3\n",
    "rdd.lookup(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**  \n",
    "This should give you a good understanding of what Pair RDDs are, how you can create them, and some common transformations."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DS5110 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
