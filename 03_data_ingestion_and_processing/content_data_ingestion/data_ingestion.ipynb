{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uva_seal.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "### University of Virginia\n",
    "### DS 5110: Big Data Systems\n",
    "### Last Updated: January 17, 2026\n",
    "\n",
    "---  \n",
    "\n",
    "### Sources \n",
    "\n",
    "Learning Spark, Chapter 9: Spark SQL\n",
    "\n",
    "### OBJECTIVES\n",
    "- Define data ingestion\n",
    "- Explain the Spark data schema and why explicitly providing it is preferable\n",
    "- Demonstrate ingestion from different file formats\n",
    "- Understand how to handle malformed records\n",
    "- Execute data partitioning\n",
    "\n",
    "### CONCEPTS AND FUNCTIONS\n",
    "- Data ingestion\n",
    "- Schema\n",
    "- Parquet files\n",
    "- Data partitioning\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data ingestion\n",
    "\n",
    "Data ingestion is the process of bringing data from external sources into a system\n",
    "\n",
    "Common **sources** include local disk, HDFS, S3, or other distributed storage\n",
    "\n",
    "Common **formats** include CSV, text files, JSON, Parquet, and Avro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"ingestion.png\" width=600> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "### 2. Data Schema in Spark\n",
    "\n",
    "The schema in Spark defines the data structure.  \n",
    "For each field, a 3-tuple is specified: `(column name, data type, nullable)`  \n",
    "\n",
    "\n",
    "\n",
    "**Example of schema with two Fields *author* and *pages*, which cannot contain null values**\n",
    "```\n",
    "schema = StructType(\n",
    "                    [StructField(\"author\", StringType(), False), \n",
    "                     StructField(\"pages\", IntegerType(), False)\n",
    "                    ])\n",
    "```\n",
    "\n",
    "You can let Spark infer the data schema, but it's preferable to feed it the schema:\n",
    "\n",
    "- Avoids having Spark launch a separate job to read a large fraction of the data to infer schema\n",
    "- Early detection of errors if the data doesn't match the schema\n",
    "- Spark inference may be incorrect. For example, it may think all numerical data are strings.\n",
    "\n",
    "**This schema is different from database schema**\n",
    "\n",
    "A database *schema* is the structure that represents the logical view of the entire database.  \n",
    "It defines how data is organized and how relations among them are associated.  \n",
    "This is implemented through the use of tables, views, and integrity constraints.\n",
    "\n",
    "**Common Spark Data Types**\n",
    "\n",
    "- Integer types, all `int` in python:\n",
    "  - ShortType\n",
    "  - IntegerType\n",
    "  - LongType\n",
    "  - FloatType\n",
    "  - DoubleType\n",
    "- StringType\n",
    "- BooleanType\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reading Files in Spark\n",
    "\n",
    "The `SparkSession.read()` method supports efficient data loading in PySpark.\n",
    "\n",
    "Several examples of batch data ingestion are demonstrated below.  \n",
    "\n",
    "**NOTES**:\n",
    "\n",
    "1 | Recall that batch data is finite data.  \n",
    "\n",
    "The process of ingesting streaming data, which is infinite data, will be discussed later in the course.\n",
    "\n",
    "2 | These examples use the *Spark DataFrame* object.\n",
    "\n",
    "See the notebook `spark_sql_and_dataframes.ipynb` to dive deeper into Spark Dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataIngestionExample\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Read CSV file\n",
    "\n",
    "`header=True` treats first row as column names\n",
    "\n",
    "`infer schema` prompts Spark to infer the schema; generally not recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = spark.read.csv(\"../data/amzn_msft_prices.csv\", header=True, inferSchema=True)\n",
    "df_csv.show(5)\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read CSV file, explicitly defining schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, TimestampType, StringType, DoubleType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"date\", TimestampType(), True),\n",
    "    StructField(\"ticker\", StringType(), True),\n",
    "    StructField(\"close\", DoubleType(), True),\n",
    "    StructField(\"adjusted_close\", DoubleType(), True),\n",
    "    StructField(\"volume\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_csv_schema = spark.read.csv(\"../data/amzn_msft_prices.csv\", header=True, schema=schema)\n",
    "df_csv_schema.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 3.2 Read semi-structured JSON file\n",
    "\n",
    "JSON supports nested structures, which PySpark can handle with structs and arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = spark.read.json(\"../data/people.json\")\n",
    "df_json.show(5)\n",
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 3.3 Reading Parquet Files\n",
    "\n",
    "- Parquet **format is columnar**\n",
    "- Reading and writing parquet files can be MUCH faster in Spark\n",
    "- It is supported by many other data processing systems\n",
    "- Stores metadata (schema) about the columns, which can provide efficiency\n",
    "- Especially useful when querying columns for analytics and ML (don't generally need entire rows of data)\n",
    "- Parquet files also have good compression options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet = spark.read.parquet(\"../data/amzn_msft_prices.parquet\")\n",
    "df_parquet.show(5)\n",
    "df_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 3.4 Handling Malformed Records\n",
    "\n",
    "Bad rows in your data can cause havok if not handled properly.\n",
    "\n",
    "In this example, there is a row with too many columns.\n",
    "\n",
    "Use option `mode` to control handling:\n",
    "- PERMISSIVE (default) → set corrupted fields to null\n",
    "- DROPMALFORMED → drop bad rows\n",
    "- FAILFAST → fail immediately if a row is bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_bad = spark.read.option(\"mode\", \"FAILFAST\") \\\n",
    "    .csv(\"../data/amzn_msft_prices_bad_row.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df_csv_bad.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv_bad = spark.read.option(\"mode\", \"DROPMALFORMED\") \\\n",
    "    .csv(\"../data/amzn_msft_prices_bad_row.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df_csv_bad.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Reading Large Datasets Efficiently\n",
    "\n",
    "For massive datasets, we want to be efficient when reading and handling data\n",
    "\n",
    "#### 4.1 Partitioning in Spark\n",
    "\n",
    "Spark reads data in parallel by splitting files into *partitions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = spark.read.option(\"header\", True).csv(\"../data/amzn_msft_prices.csv\")\n",
    "print(df_csv.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repartition** for better parallelism on large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repart = df_csv.repartition(6)  # 6 partitions\n",
    "print(df_repart.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Column pruning**: Read only necessary columns to reduce memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select = df_csv.select(\"date\",\"ticker\",\"adjusted_close\")\n",
    "df_select.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 4.2 Partition Discovery\n",
    "\n",
    "We looked at partitioning data in Spark.\n",
    "\n",
    "Database tables can also be partitioned to make querying more efficient.  \n",
    "\n",
    "For example, a dataset may have logical groupings, such as locations or demographic subsets. \n",
    "\n",
    "We might split the data by **gender** and **country**, producing smaller tables.  \n",
    "\n",
    "If the analyst queries a country against the partitioned table, it will run faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different Directories**\n",
    "\n",
    "In a partitioned table, data are usually stored in different directories, with partitioning column values encoded in the path of each partition directory.  \n",
    "\n",
    "All built-in file sources (including Text/CSV/JSON/ORC/Parquet) are able to discover and infer partitioning information automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path\n",
    "└── to\n",
    "    └── table\n",
    "        ├── gender=male\n",
    "        │   ├── ...\n",
    "        │   │\n",
    "        │   ├── country=US\n",
    "        │   │   └── data.parquet\n",
    "        │   ├── country=CN\n",
    "        │   │   └── data.parquet\n",
    "        │   └── ...\n",
    "        └── gender=female\n",
    "            ├── ...\n",
    "            │\n",
    "            ├── country=US\n",
    "            │   └── data.parquet\n",
    "            ├── country=CN\n",
    "            │   └── data.parquet\n",
    "            └── ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples of writing DF to Parquet file, partitioning columns**\n",
    "\n",
    "```\n",
    "df = df.withColumn('end_month', F.month('end_date'))\n",
    "df = df.withColumn('end_year', F.year('end_date'))\n",
    "df.write.partitionBy(\"end_year\", \"end_month\").parquet(\"/tmp/sample_table\")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY FOR YOURSELF (UNGRADED EXERCISES)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Write and execute code to ingest the JSON file `\"../data/people.json\"` and store the data in the `\\data` folder in Parquet format.  \n",
    "   If things completed successfully, you will see a `people.parquet` folder with the file `_SUCCESS` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. Summary\n",
    "\n",
    "You should now have some understanding of how to ingest different data formats into Spark.\n",
    "\n",
    "We also covered some methods for efficiently handling big data.\n",
    "\n",
    "Next, dive deeper into Spark Dataframes and Spark SQL in `spark_sql_and_dataframes.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "PySpark 4.0.0",
   "language": "python",
   "name": "pyspark-4.0.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
