{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uva_seal.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "### University of Virginia\n",
    "### DS 5110: Big Data Systems\n",
    "### Last Updated: February 1, 2023\n",
    "\n",
    "---  \n",
    "\n",
    "\n",
    "### SOURCES\n",
    "Learning Spark 1st Ed., Chapter 3: Programming with RDDs   \n",
    "\n",
    "### OBJECTIVES\n",
    "-  Basics of RDDs including transformations and actions  \n",
    "-  Discuss parallelization concepts  \n",
    "\n",
    "\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- RDD  \n",
    "- Transformation  \n",
    "- Action  \n",
    "- lazy evaluation  \n",
    "- SparkSession\n",
    "- Lineage graph - Graph of dependencies between all involved RDDs  \n",
    "- Set operations  \n",
    "- Pipelining or chaining  \n",
    "- accumulator  \n",
    "- `persist()` and `cache()`  \n",
    "- `parallelize()`  \n",
    "- `collect()` and `take()`  \n",
    "- `map()`, `filter()`, `flatMap()`  \n",
    "- `reduce()`, `fold()`, `aggregate()`  \n",
    "- `count()`, `countByValue()`  \n",
    "- `saveAsTextFile()`, `saveAsSequenceFile()`  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD BASICS\n",
    "\n",
    "An *RDD* is a distributed collection of elements  \n",
    "It is the most basic abstraction in Spark, created at the birth of Spark.\n",
    "\n",
    "All work consists of:  \n",
    "- RDD creation  \n",
    "- RDD transformation  \n",
    "- RDD action (e.g., compute a result)  \n",
    "\n",
    "**When RDDs are Useful**\n",
    "- For unstructured data like documents  \n",
    "- Certain models and applications require them\n",
    "\n",
    "---\n",
    "**ASIDE**  \n",
    "\n",
    "DataFrames are more useful for structured data (e.g., tabular)  \n",
    "We study them later  \n",
    "Under the hood, DataFrames are constructed as rows of RDDs\n",
    "\n",
    "---\n",
    "\n",
    "Spark “magically” handles distributing data and code across cluster, parallelization of operations\n",
    "\n",
    "Spark is \"lazy.\"  \n",
    "It doesn’t actually do any work until it encounters an *action*, for example a `count()`.  \n",
    "\n",
    "Spark creates a logical plan or roadmap to optimize performance of the project.  \n",
    "This is called a *directed acyclic graph* (DAG).  \n",
    "It makes these optimizations without help from the user.  \n",
    "\n",
    "**Example DAG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dag.png\" size=100> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When testing/debugging code, it can be helpful to call `count()` to force Spark to evaluate results.  \n",
    "This gives a sense of what breaks and how long things take.  \n",
    "\n",
    "A *transformation* creates a new RDD  \n",
    "\n",
    "An *action* returns a different data type  \n",
    "\n",
    "RDDs are created in two ways:   \n",
    "1. Loading external dataset (`textFile()`, for example)\n",
    "2. Distributing a collection of objects from driver program  \n",
    "\n",
    "for example:\n",
    "```\n",
    "nums = sc.parallelize([1,2,3,4])\n",
    "```\n",
    "\n",
    "The `SparkSession` is a single entry point for working with Spark.  \n",
    "It was created in Spark 2.0 to unify and simplify multiple context managers.  \n",
    "For working with RDDs, the `Spark Context` is required. It is an attribute in SparkSession.  \n",
    "The example below illustrates their use. \n",
    "\n",
    "**Example of Transformation: Filter on text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Spark Session from pyspark.sql library\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create SparkSession entry point\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Spark Context is needed for working with RDDs. Extract it from Spark Session\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Read in a text file\n",
    "lines = sc.textFile(\"README.txt\")\n",
    "\n",
    "# Filter the data by applying a lambda function\n",
    "pythonLines = lines.filter(lambda line: \"Python\" in line)\n",
    "\n",
    "# Collect the filtered data to the driver\n",
    "py = pythonLines.collect()\n",
    "\n",
    "# For each line of text, print the index and text\n",
    "for i, p in enumerate(py):\n",
    "    print('line: {} text: {}'.format(i,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Operations on RDDs\n",
    "\n",
    "Store or “persist” an RDD by calling  \n",
    "\n",
    "`RDD.persist()` \n",
    "\n",
    "`cache()` is the same as `persist()` with the default storage level\n",
    "\n",
    "`collect()`  \n",
    "Retrieve entire RDD on driver.  \n",
    "Careful w large RDDs, as the results need to fit in memory on single machine!\n",
    "\n",
    "`take()`  \n",
    "Retrieve small number of elements from RDD (user can specify size).  \n",
    "NOTE: values may NOT be in order\n",
    "\n",
    "`first()`  \n",
    "Retrieve first element from RDD  \n",
    "\n",
    "`saveAsTextFile()`, `saveAsSequenceFile()`, `…`  \n",
    "Save contents of RDD as a file. Different function call depending on file storage type.\n",
    "\n",
    "\n",
    "### Some Basic Transformations \n",
    "\n",
    "`map()`  \n",
    "Applies transform to each element in RDD  \n",
    "\n",
    "`flatMap()`  \n",
    "Apply map to produce list of elements in a single list (e.g, tokenize a sentence into words)  \n",
    "\n",
    "`filter()`  \n",
    "Return new RDD with only records meeting condition\n",
    "\n",
    "`parallellize()`  \n",
    "Distribute the data to workers, creating an RDD  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of text processing with `map`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in a text file. It happens to be pipe delimited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = sc.textFile(\"pipe_delim_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the columns by splitting on pipe delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_clean = pipe.map(lambda x: x.split('|'))\n",
    "pipe_clean.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the split converts strings to lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps you want only a subset of the columns, like the first and third columns.    \n",
    "The first `map` splits the strings, and the second `map` does the subsetting, placing data into tuples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_clean_first_two = pipe.map(lambda x: x.split('|')) \\\n",
    "                           .map(lambda x: (x[0], x[2]))\n",
    "\n",
    "pipe_clean_first_two.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity of Transformations\n",
    "\n",
    "Some transformations are simpler than others.\n",
    "\n",
    "For example, `filter()` can operate independently on each data partition, and the results can be combined. This is sometimes called a *narrow transformation*.\n",
    "\n",
    "Computing a median on the data, however, is more complex, since it requires ordering all the data. This means data would need to be shuffled across the cluster, which is an expensive operation.  This is sometimes called a *wide transformation*.\n",
    "\n",
    "It is preferable to keep transformations as simple as possible.\n",
    "\n",
    "**Example of Distributing Data to Workers with `parallelize()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "nums = sc.parallelize([1,2,3,4])\n",
    "\n",
    "# Show this object is RDD data type\n",
    "print(type(nums))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = sc.parallelize(['cat','dog','baby'])\n",
    "list2 = sc.parallelize(['giraffe','baby'])\n",
    "\n",
    "# take the union of two RDDs\n",
    "list1.union(list2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice this does not filter duplicates  \n",
    "\n",
    "Also notice we can “chain” or “pipeline” commands in sequence  \n",
    "\n",
    "Let’s get the distinct list from the union:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1.union(list2).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: `distinct()` is expensive as it requires shuffling all data over the network  \n",
    "\n",
    "Shuffling: the process of redistributing data across partitions  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions\n",
    "\n",
    "`reduce()`  \n",
    "Process elements into a new element of the same type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an RDD and sum the integers, two at a time\n",
    "\n",
    "l1 = sc.parallelize([1,2,3,4])\n",
    "sum = l1.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sum: {}'.format(sum))\n",
    "print('l1 type: {}'.format(type(l1)))\n",
    "print('sum type: {}'.format(type(sum)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fold()`  \n",
    "Similar to `reduce()`, includes “zero value” acting as identity  \n",
    "\n",
    "`aggregate()`  \n",
    "Similar to reduce and fold, uses:  \n",
    "1. initial value \n",
    "2. combining function for each worker or node\n",
    "3. combining function to merge results across workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`countbyValue()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = sc.parallelize([1,2,3,3,4])\n",
    "cv = nums.countByValue()\n",
    "\n",
    "print('cv[1]: {}'.format(cv[1]))\n",
    "print('cv[3]: {}'.format(cv[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY FOR YOURSELF (UNGRADED EXERCISES)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Use `reduce()` to compute the cumulative product of the odd numbers from 1 through 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Print the intersection between these two RDDs, and `collect()` the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize(['cat','dog','baby'])\n",
    "rdd2 = sc.parallelize(['giraffe','baby','baby'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Print the elements in `rdd1` that are NOT in `rdd2`. The `subtract()` function can be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1)\n",
    "\n",
    "l1 = sc.parallelize([val for val in range(1,17,2)])\n",
    "cumprod = l1.reduce(lambda x,y: x*y)\n",
    "cumprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2) \n",
    "\n",
    "rdd1 = sc.parallelize(['cat','dog','baby'])\n",
    "rdd2 = sc.parallelize(['giraffe','baby','baby'])\n",
    "rdd1.intersection(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3)\n",
    "\n",
    "rdd1.subtract(rdd2).collect()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DS5110 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
