{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uva_seal.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib Regression\n",
    "\n",
    "### University of Virginia\n",
    "### DS 5110: Big Data Systems\n",
    "### Last Updated: March 2, 2023\n",
    "\n",
    "---  \n",
    "\n",
    "\n",
    "### SOURCES\n",
    "Learning Spark: Machine Learning with MLlib\n",
    "\n",
    "*Details on regularization equation*  \n",
    "https://spark.apache.org/docs/1.5.2/ml-linear-methods.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-regression\n",
    "\n",
    "\n",
    "### OBJECTIVES\n",
    "- Introduction to major regression models in MLlib using the DataFrame API\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Linear regression\n",
    "- VectorAssembler\n",
    "- RegressionEvaluator\n",
    "\n",
    "---\n",
    "\n",
    "### Introduction to Regression\n",
    "\n",
    "Earlier, we discussed the classification problem where the response variable is discrete\n",
    "\n",
    "Regression is another common form of supervised learning\n",
    "The response variable in a regression problem is quantitative or continuous  \n",
    "\n",
    "Several classification models also have regression counterparts, including:\n",
    "\n",
    "- Support vector machines  \n",
    "- Tree-based methods like random forests and gradient-boosted trees  \n",
    "\n",
    "**The distributed processing used by Spark is particularly helpful in random forests.**  \n",
    "The trees can be distributed across executors and built, and the results can be aggregated.\n",
    "\n",
    "To implement the regression counterpart, the same package is loaded but a different method is called.\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "Linear regression is the most fundamental model used in regression.\n",
    "\n",
    "Model assumes a linear relationship between a set of explanatory variables $X$ (aka features, factors, predictors, independent variables) and a scalar response variable $Y$.\n",
    "\n",
    "Linear regression models are most often fit using the *ordinary least squares* (*OLS*) approach.  \n",
    "\n",
    "A regularization term is often added to the loss function to help with generalization. Examples include:\n",
    "\n",
    "- ridge regression ($L^2$-norm penalty)\n",
    "- lasso ($L^1$-norm penalty)\n",
    "- elastic net is a blend of ridge and lasso regression\n",
    "\n",
    "#### Linear Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/03 02:14:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|median_house_value|median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|          452600.0|       8.3252|              41.0|      880.0|         129.0|     322.0|     126.0|   37.88|  -122.23|\n",
      "|          358500.0|       8.3014|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|   37.86|  -122.22|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"mllib_classifier\").getOrCreate()\n",
    "\n",
    "# Load training data\n",
    "filename = \"sample_housing_data.csv\"\n",
    "training = spark.read.csv(filename,  inferSchema=True, header = True)\n",
    "training.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In DataFrame API version of MLlib, features need to be assembled into a feature column for the ML \n",
    "Model\n",
    "\n",
    "`VectorAssembler` will handle this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+--------------+\n",
      "|median_house_value|median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|features      |\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+--------------+\n",
      "|452600.0          |8.3252       |41.0              |880.0      |129.0         |322.0     |126.0     |37.88   |-122.23  |[8.3252,880.0]|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# inputCols take a list of column names\n",
    "# outputCol is arbitrary name of new column; generally called features\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"median_income\", \"total_rooms\"],\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "tr = assembler.transform(training)\n",
    "tr.select(\"*\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set up the Linear Regression Model and fit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [19866.979640214464,-9.836075133390931]\n",
      "Intercept: 261023.86960904166\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol='features',         # feature vector name\n",
    "                      labelCol='median_house_value',  # target variable name\n",
    "                      maxIter=10,\n",
    "                      regParam=0.3, \n",
    "                      elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(tr)\n",
    "\n",
    "# Print the weights and intercept for linear regression\n",
    "print(\"Weights: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's measure the model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+--------------+------------------+\n",
      "|median_house_value|median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|      features|        prediction|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+--------------+------------------+\n",
      "|          452600.0|       8.3252|              41.0|      880.0|         129.0|     322.0|     126.0|   37.88|  -122.23|[8.3252,880.0]|417764.70239237114|\n",
      "+------------------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+--------------+------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "--------------------\n",
      "METRICS\n",
      "Mean Squared Error: 703796169.4197679\n",
      "R Squared: 0.6032614088503244\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# compute predictions. this will append column \"prediction\" to dataframe\n",
    "lrPred = lrModel.transform(tr)\n",
    "lrPred.show(1)\n",
    "\n",
    "ev = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"median_house_value\")\n",
    "\n",
    "print('-'*20)\n",
    "print(\"METRICS\")\n",
    "print(\"Mean Squared Error:\", ev.evaluate(lrPred, {ev.metricName: \"mse\"}))\n",
    "print(\"R Squared:\", ev.evaluate(lrPred, {ev.metricName:'r2'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "Notice we extracted a metric like MSE by: \n",
    "1. passing to the evaluator a dataframe with labels and predictions\n",
    "2. going to the Regression Evaluator dictionary with the desired key: \"mse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```ev.evaluate(lrPred, {ev.metricName: \"mse\"})```\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization Parameters\n",
    "\n",
    "You might have noticed the parameters `regParam` and `elasticNetParam` in the function call above.  \n",
    "You can read about them [here](https://spark.apache.org/docs/1.5.2/ml-linear-methods.html).  \n",
    "\n",
    "The `elasticNetParam` parameter controls the relative blending of Lasso and Ridge regression.  \n",
    "These regularization terms often help a model better generalize to new data by reducing overfitting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Regression Models using the DataFrame API\n",
    "\n",
    "PySpark supports several regression models including:  \n",
    "- Generalized linear regression\n",
    "- Decision tree regression\n",
    "- Random forest regression\n",
    "- Gradient-boosted tree regression\n",
    "\n",
    "For more details, including code examples, please see [here](https://spark.apache.org/docs/latest/ml-classification-regression.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY FOR YOURSELF (UNGRADED EXERCISES)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Copy the Linear Regression example from above, and modify in the cells below to fit and evaluate these two models:  \n",
    "\n",
    "i. Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Think of at least one real-world example of when you would need to implement each of the following tasks:  \n",
    "- regression\n",
    "- binary classification\n",
    "- multiclass classification\n",
    "- multilabel classification\n",
    "\n",
    "If you are not sure about the difference between multiclass and multilabel, here is one resource:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/multiclass.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DS5110 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
