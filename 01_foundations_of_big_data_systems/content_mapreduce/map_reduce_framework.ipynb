{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uva_seal.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce Framework\n",
    "\n",
    "### University of Virginia\n",
    "### DS 5110: Big Data Systems\n",
    "### Last Updated: January 14, 2026\n",
    "\n",
    "---  \n",
    "\n",
    "### SOURCE\n",
    "\n",
    "Hadoop: The Definitive Guide, Tom White\n",
    "\n",
    "### OBJECTIVES\n",
    "-  Introduction to the MapReduce framework\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Centralized vs Decentralized vs Distributed Data\n",
    "- MapReduce\n",
    "- Jobs\n",
    "- Tasks\n",
    "- Map operation\n",
    "- Reduce operation\n",
    "- Shuffle operation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Centralized vs Decentralized vs Distributed Data\n",
    "\n",
    "We start by explaining *centralized*, *decentralized*, and *distributed data*.\n",
    "\n",
    "In this course, we focus on distributed data.\n",
    "\n",
    "Imagine we have a large amount of data and we are deciding how it should be stored and managed.\n",
    "\n",
    "*Centralized data* has a single control point (it may live on a single large machine).  \n",
    "Managing this data may be relatively easy, but there is a single failure point.  \n",
    "It also won't scale.\n",
    "\n",
    "*Decentralized data* is spread across machines (nodes).   \n",
    "A decentralized system has **no single controlling authority**. Each node is autonomous and may have its own goals, policies, or owners.  \n",
    "This can help scale, but it will be more complex to manage.  \n",
    "An example would be a set of iPhones collecting data, where no single server collects and manages the data.  \n",
    "\n",
    "*Distributed data* is spread across nodes.  \n",
    "There is a **single controlling authority**, which helps coordinate activities.  \n",
    "Nodes cooperate to act like one large computer.  \n",
    "Spark is a distributed system.\n",
    "\n",
    "<img src=\"data_pattern.png\" width=600>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Introducing MapReduce\n",
    "\n",
    "`MapReduce` is a programming framework for processing large data sets with a parallel, **distributed algorithm** on a cluster.  \n",
    "\n",
    "First the data set is “mapped” into a collection of `<key, value>` pairs, and then “reduced” over all pairs with the same key.  \n",
    "We need to discuss what this means.  \n",
    "\n",
    "The operations are surprisingly broad, enabling them to handle a wide range of use cases.\n",
    "\n",
    "The popular minimal example is to do a word count on some text.\n",
    "\n",
    "Initially published in 2004 by employees at Google\n",
    "\n",
    "A `MapReduce` job is a unit of work the client wants to be performed  \n",
    "\n",
    "A job consists of:  \n",
    "\n",
    "- Input data\n",
    "- `MapReduce` program\n",
    "- configs\n",
    "\n",
    "The job will be divided into *tasks*  \n",
    "Two types of tasks: *map tasks* and *reduce tasks*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside on `Hadoop`**  \n",
    "\n",
    "`MapReduce` forms the computation paradigm for Hadoop  \n",
    "\n",
    "`Hadoop` was created by Doug Cutting and Mike Cafarella.  Version 1 was called Nutch.  \n",
    "\n",
    "Yahoo! was instrumental in providing a dedicated team and resources to turn Hadoop into a system that ran at web scale.\n",
    "\n",
    "Hadoop is developed and maintained by the Apache Software Foundation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside on Hadoop and Spark**  \n",
    "\n",
    "Spark does not follow the `MapReduce` framework in the same way that `Hadoop` does, where `Mapper` and `Reducer` functions are detailed explicitly.  \n",
    "\n",
    "This is actually a big advantage of Spark.\n",
    "\n",
    "Spark and Hadoop can be better together.  For example, some teams will use Hadoop data storage (HDFS) with Spark.  \n",
    "That is, a Spark job can read data from HDFS, and write results to HDFS.\n",
    "\n",
    "Reported runtimes:\n",
    "Spark can be as much as 10x faster than `MapReduce` for batch processing, and up to 100x faster for in-memory analytics.\n",
    "\n",
    "The backbone of this course will cover Spark (in particular, PySpark) for various tasks.\n",
    "\n",
    "Next we illustrate the `MapReduce` framework with an example.\n",
    "\n",
    "---\n",
    "\n",
    "**MapReduce Word Count Process Diagram**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"map_reduce_example2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some Things to Notice:**\n",
    "\n",
    "**Splitting Step**  \n",
    "A given number of records is assigned to each worker\n",
    "\n",
    "NOTE:  \n",
    "More splits means less processing time per split  \n",
    "However, the overhead of managing splits and `map` task creation starts to dominate total job execution time  \n",
    "\n",
    "**Mapping Step**  \n",
    "Each token (word) is given a value of 1 representing the count.  \n",
    "The token, count forms a `<key, value> pair.`  \n",
    "If the same pair appears $n$ times on a worker, there would be $n$ entries of `<map, 1>`.  In other words, no aggregation is done at this step (it happens at the `reduce` step).  \n",
    "\n",
    "**Shuffling Step**  \n",
    "The `<key, value>` pairs are moved across machines at this point, so that pairs with the same key are gathered on the same machine.  This is a costly step in the process.\n",
    "\n",
    "**Reducing Step**  \n",
    "A `reducer` applies a given operation over the values by key.  For this word count example, the counts would be summed for each key and stored with the key.\n",
    "\n",
    "**Map and Reduce in General**  \n",
    "\n",
    "**Map**  \n",
    "Higher-order function that applies a function to each element of a list (e.g., $x => x**2$)\n",
    "\n",
    "**Reduce**  \n",
    "Higher-order function that applies a combining operation (e.g,. cumulative sum of values)\n",
    "\n",
    "**Combiner Functions**  \n",
    "It can be expensive to shuffle data within the cluster, and the limiting factor is the bandwidth on the cluster.\n",
    "\n",
    "`Combiner Functions` can be run on the map output, before the shuffle occurs. This acts locally on each node, and reduces some of the data transfer.  Note this won’t always work (e.g., can’t be used for computing the average, but CAN be used for computing the maximum).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. MapReduce Code Illustration\n",
    "\n",
    "Next, we look at how PySpark implements the word count program.  \n",
    "\n",
    "**Word Count Example**  \n",
    "Data source: Spark README file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# context manager: minimal settings\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list files in the directory\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "lines = sc.textFile(\"README.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some records\n",
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show top 10 most frequent words** (we will examine in detail later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda x: x.split())\n",
    "wordcounts = words.map(lambda x: (x, 1)) \\\n",
    "                  .reduceByKey(lambda x,y:x+y) \\\n",
    "                  .map(lambda x:(x[1],x[0])) \\\n",
    "                  .sortByKey(False)\n",
    "wordcounts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: How are `map()` and `flatMap()` different?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribute data to the workers\n",
    "data = sc.parallelize([3,4,5])\n",
    "\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize([3,4,5]).map(lambda x: [x,  x*x]).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize([3,4,5]).flatMap(lambda x: [x, x*x]).collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map: a list of lists  \n",
    "flatMap: a flattened list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**TRY FOR YOURSELF (UNGRADED EXERCISES)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Copy the word count code into the cell below.  Modify it to print the top 20 wordcount pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Copy the word count code into the cell below.  Modify it to filter out rows containing the word *Spark* and then execute the word count, returning the top 10 wordcount pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1)\n",
    "\n",
    "wordcounts = lines.flatMap(lambda x: x.split()) \\\n",
    "            .map(lambda x: (x, 1)) \\\n",
    "            .reduceByKey(lambda x,y:x+y) \\\n",
    "            .map(lambda x:(x[1],x[0])) \\\n",
    "            .sortByKey(False)\n",
    "\n",
    "wordcounts.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2)\n",
    "\n",
    "wordcounts = lines.filter(lambda x: \"Spark\" not in x) \\\n",
    "            .flatMap(lambda x: x.split()) \\\n",
    "            .map(lambda x: (x, 1)) \\\n",
    "            .reduceByKey(lambda x,y:x+y) \\\n",
    "            .map(lambda x:(x[1],x[0])) \\\n",
    "            .sortByKey(False)\n",
    "\n",
    "wordcounts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
