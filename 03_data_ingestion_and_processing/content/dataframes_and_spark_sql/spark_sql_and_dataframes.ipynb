{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uva_seal.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL and DataFrames\n",
    "\n",
    "### University of Virginia\n",
    "### DS 5110: Big Data Systems\n",
    "### Last Updated: January 17, 2026\n",
    "\n",
    "---  \n",
    "\n",
    "### Sources \n",
    "\n",
    "Learning Spark, Chapter 9: Spark SQL\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning\n",
    "\n",
    "Demonstration of several useful DataFrame operations:  \n",
    "https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html\n",
    "\n",
    "### OBJECTIVES\n",
    "- Introduction to Spark SQL, the interface for working with structured and semistructured data\n",
    "- Introduce DataFrames and show basic functionality\n",
    "- Implement queries using Spark SQL\n",
    "- Execute tasks including filtering and aggregations using Spark DataFrames\n",
    "\n",
    "### CONCEPTS AND FUNCTIONS\n",
    "- Spark SQL\n",
    "- Temp table\n",
    "- Dataset and DataFrame\n",
    "- Aggregations with groupBy()\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Overview\n",
    "\n",
    "Two important ways of working with big data in Spark: \n",
    "\n",
    "- Through Spark SQL\n",
    "- Using DataFrames\n",
    "\n",
    "They also interoperate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SQL in Ten Seconds (tongue in cheek)\n",
    "\n",
    "\n",
    "SQL is a structured query language used to communicate with relational databases.  \n",
    "Commands include CREATE, SELECT, UPDATE, ALTER, INSERT INTO, DROP, DELETE.  \n",
    "This course will use SELECT.\n",
    "\n",
    "### 3. Spark SQL Capabilities:\n",
    "\n",
    "- Load data from various structured formats including JSON, Hive, Parquet  \n",
    "- Query data using SQL inside Spark or from external tools that connect to Spark (e.g., `Tableau`) \n",
    "- Spark SQL integrates between SQL and Python/Java/Scala/R code. Can do things like join RDDs and SQL tables.\n",
    "\n",
    "### 4. Note on Spark SQL Development\n",
    "\n",
    "- Spark SQL has been heavy development area in new releases \n",
    "- As the module involves massive amounts of data, optimizing operations is valuable\n",
    " \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Dataset and DataFrame\n",
    "\n",
    "- A Dataset is a distributed collection of data   \n",
    "- A Dataset can be constructed from JVM objects\n",
    "- It can be manipulated using functional transformations (`map()`, `flatMap()`, `filter()`, etc.)  \n",
    "- A DataFrame is a Dataset organized into named columns   \n",
    "\n",
    "In practice, you will be thinking in terms of `DataFrames`, and not `Datasets`.  \n",
    "\n",
    "For users familiar with dataframes from R and Python, they are similar, yet with operations distinct to Spark.  \n",
    "\n",
    "As an example, adding a new column to a DataFrame is executed using `withColumn()`.  \n",
    "\n",
    "This may feel more formal compared to R and Python.  \n",
    "\n",
    "Additionally - when compared to R and Python: \n",
    "\n",
    "**The Spark DataFrame is built up from RDDs.    \n",
    "It inherits the properties of distributed data, lazy execution, DAGs.**  \n",
    "\n",
    "DataFrames can be constructed from a wide array of sources such as:  \n",
    "structured data files, tables in Hive, external databases, or existing RDDs.  \n",
    "\n",
    "The DataFrame API is available in Scala, Java, Python, and R. \n",
    "\n",
    "---\n",
    "\n",
    "### 6. DataFrames vs RDDs  \n",
    "\n",
    "When is it better to use DataFrames, and when is it better to use RDDs?  \n",
    "\n",
    "Here are some recommendations:   \n",
    "\n",
    "- In general, most work can be done with DataFrames  \n",
    "\n",
    "- Use DataFrames to use high-level expressions, to perform SQL queries to explore the data, and to gain columnar access.\n",
    "\n",
    "**When to use DataFrames**\n",
    "\n",
    "- If you are thinking about the data by field names, you probably want the data in a DataFrame\n",
    "\n",
    "- For machine learning and building predictive models, DataFrames are recommended.  \n",
    "  You will be exploring the data by column, and building features from the columns of data.\n",
    "\n",
    "**When to use RDDs**\n",
    "  \n",
    "- RDDs can be useful to perform low-level transformations and actions on unstructured data.  \n",
    "  For example, filtering strings and performing other simple transformations on text is best done with RDDs.  \n",
    "  In these cases, the analyst doesn't care about field names, and there is no need to impose schema on the data.  \n",
    "\n",
    "- Use RDDs when you want to manipulate the data with functional programming constructs rather than domain specific expressions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Creating a DataFrame\n",
    "\n",
    "There are multiple ways to do this:\n",
    "- use a function such as `read.csv()` to read data from files into DataFrames (most common)\n",
    "- pass data to `createDataFrame()` with schema\n",
    "- conversion from RDD using `toDF()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1: Create DataFrame from RDD using `toDF()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "```\n",
    "# import modules \n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Map the RDD to a DF\n",
    "\n",
    "df = rdd.map(lambda line: Row(longitude=line[0], \n",
    "                              latitude=line[1], \n",
    "                              housingMedianAge=line[2],\n",
    "                              totalRooms=line[3],\n",
    "                              totalBedRooms=line[4],\n",
    "                              population=line[5], \n",
    "                              households=line[6],\n",
    "                              medianIncome=line[7],\n",
    "                              medianHouseValue=line[8])).toDF()\n",
    "```\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2: Create DataFrame by passing data and schema to `createDataFrame()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------+\n",
      "|id |sentence                       |\n",
      "+---+-------------------------------+\n",
      "|0  |ChatGPT is all the rage        |\n",
      "|1  |Google released BARD to compete|\n",
      "|2  |What does AWS think about this?|\n",
      "+---+-------------------------------+\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# import context manager: SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# import data types\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# set up the session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# create some data; list of tuples\n",
    "data = [\n",
    "    (0, \"ChatGPT is all the rage\"),\n",
    "    (1, \"Google released BARD to compete\"),\n",
    "    (2, \"What does AWS think about this?\")\n",
    "]\n",
    "\n",
    "# define schema; each field holds (name, data type, nullable)\n",
    "# for large number of fields, best to automate schema construction\n",
    "schema = StructType([StructField('id', IntegerType(), False), \n",
    "                     StructField('sentence', StringType(), False)])\n",
    "\n",
    "# create df by passing data, schema\n",
    "sentenceDataFrame = spark.createDataFrame(data, schema)\n",
    "\n",
    "# print first few records\n",
    "sentenceDataFrame.show(3, False)\n",
    "\n",
    "# print data type\n",
    "print(type(sentenceDataFrame))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3: Create a DataFrame from some JSON data with spark.read()**  \n",
    "(For an example of JSON data see: http://json.org/example.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Read data in json format\n",
    "df = spark.read.json(\"../data/people.json\")\n",
    "\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating an RDD from a DataFrame**\n",
    "\n",
    "This is very simple: `df.rdd`\n",
    "\n",
    "Here we convert our df containing sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=0, sentence='ChatGPT is all the rage'), Row(id=1, sentence='Google released BARD to compete')]\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "sentence_rdd = sentenceDataFrame.rdd\n",
    "print(sentence_rdd.take(2))\n",
    "print(type(sentence_rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 8. Some Useful DataFrames Operations\n",
    "\n",
    "Next we explore subsetting, filtering, and aggregation among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data in json format\n",
    "df = spark.read.json(\"../data/people.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting\n",
    "\n",
    "There are three ways to select columns and we show them all. Find your favorite!\n",
    "\n",
    "- Bracket operator\n",
    "- `col()` method\n",
    "- Dot operator (my favorite)\n",
    "\n",
    "We see them below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep records where age > 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bracket operator\n",
    "df.filter(df['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dot operator\n",
    "df.filter(df.age > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# column operator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter(col('age') > 21).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep records subject to filters on name, then sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import asc\n",
    "\n",
    "# alternatively using df.name instead of col(\"name\")\n",
    "df.filter((df.name == \"Andy\") | (df.name == \"Michael\")).sort(asc(\"name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Fetch records with age *null*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"age\").isNull()).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch records with age *not null*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 30|  Andy|\n",
      "| 19|Justin|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"age\").isNotNull()).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### where() is equivalent to filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where((df.name == \"Andy\") | (df.name == \"Michael\")).sort(asc(\"name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing with 0 (just for illustration; not a great idea for this data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "|  0|Michael|\n",
      "| 30|   Andy|\n",
      "| 19| Justin|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the age field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               age|\n",
      "+-------+------------------+\n",
      "|  count|                 2|\n",
      "|   mean|              24.5|\n",
      "| stddev|7.7781745930520225|\n",
      "|    min|                19|\n",
      "|    max|                30|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe(\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 9. Spark SQL Queries\n",
    "\n",
    "The interface of SQL and DataFrames.    \n",
    "To write SQL queries against DataFrames, first register DF as a `SQL temp view`, and then write the query.  \n",
    "A temp view does not persist; it only lasts until the session ends.\n",
    "\n",
    "\n",
    "**Example of SQL Query against DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# register DataFrame as temp view with name \"people\"\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# query the view\n",
    "sqlDF = spark.sql(\"SELECT * FROM people where name == 'Andy'\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 10. Aggregate on columns\n",
    "\n",
    "SQL functions can be loaded from this library: `pyspark.sql.functions`\n",
    "\n",
    "We will load some stock data to demonstrate aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data types and functions\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, StringType, LongType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# set up schema\n",
    "stock_schema = StructType([StructField('date',TimestampType(),False),\n",
    "                           StructField('ticker',StringType(),False),\n",
    "                           StructField('close',FloatType(),False),\n",
    "                           StructField('adjusted_close',FloatType(),False),\n",
    "                           StructField('volume',LongType(),False),\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+--------------+--------+\n",
      "|      date|ticker| close|adjusted_close|  volume|\n",
      "+----------+------+------+--------------+--------+\n",
      "|2022-02-10|  MSFT|302.38|     299.57297|45386200|\n",
      "|2022-02-11|  MSFT|295.04|     292.30112|39175600|\n",
      "|2022-02-14|  MSFT| 295.0|      292.2615|36359500|\n",
      "|2022-02-15|  MSFT|300.47|      297.6807|27058300|\n",
      "|2022-02-16|  MSFT| 299.5|     297.33322|29982100|\n",
      "|2022-02-17|  MSFT|290.73|     288.62668|32461600|\n",
      "|2022-02-18|  MSFT|287.93|     285.84692|34264000|\n",
      "|2022-02-22|  MSFT|287.72|     285.63846|41736100|\n",
      "|2022-02-23|  MSFT|280.27|     278.24234|37811200|\n",
      "|2022-02-24|  MSFT|294.59|     292.45874|56989700|\n",
      "|2022-02-25|  MSFT|297.31|     295.15906|32546700|\n",
      "|2022-02-28|  MSFT|298.79|     296.62836|34627500|\n",
      "|2022-03-01|  MSFT|294.95|     292.81613|31217800|\n",
      "|2022-03-02|  MSFT|300.19|     298.01822|31873000|\n",
      "|2022-03-03|  MSFT|295.92|     293.77914|27314500|\n",
      "|2022-03-04|  MSFT|289.86|     287.76294|32356500|\n",
      "|2022-03-07|  MSFT|278.91|     276.89218|43157200|\n",
      "|2022-03-08|  MSFT|275.85|      273.8543|48159500|\n",
      "|2022-03-09|  MSFT| 288.5|      286.4128|35204500|\n",
      "|2022-03-10|  MSFT|285.59|     283.52383|30628000|\n",
      "+----------+------+------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in stock data. Source: Yahoo! finance\n",
    "DATAPATH_STOCKS = '../data/amzn_msft_prices.csv'\n",
    "\n",
    "df_stx = spark.read.csv(DATAPATH_STOCKS, header=True, schema=stock_schema)\n",
    "df_stx.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- ticker: string (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- adjusted_close: float (nullable = true)\n",
      " |-- volume: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the schema\n",
    "df_stx.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will calculate some statistics on each stock:\n",
    "- minimum closing price\n",
    "- maximum closing price\n",
    "- minimum volume\n",
    "- maximum volume\n",
    "\n",
    "**IMPORTANT NOTE**  \n",
    "Do NOT use loops to aggregate data. Loops are run sequentially and do not harness parallelization.  \n",
    "Using the `groupBy()` method will do the job using parallelization.  \n",
    "It follows the split, apply, combine method:\n",
    "\n",
    "- *Split* by one or more grouping variables\n",
    "- *Apply* a function to the data parts\n",
    "- *Combine* the result from each grouping level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+-----------+-----------+\n",
      "|ticker|min(close)|max(close)|min(volume)|max(volume)|\n",
      "+------+----------+----------+-----------+-----------+\n",
      "|  AMZN|     81.82|   169.315|   35088600|  272662000|\n",
      "|  MSFT|    214.25|    315.41|    9200800|   86102000|\n",
      "+------+----------+----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agg_df = df_stx.groupBy(\"ticker\").agg(F.min(\"close\"), F.max(\"close\"), F.min(\"volume\"), F.max(\"volume\"))\n",
    "agg_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**TRY FOR YOURSELF (UNGRADED EXERCISES)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Given the stock dataframe, use Spark SQL to select all AMZN records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+--------------+---------+\n",
      "|      date|ticker|   close|adjusted_close|   volume|\n",
      "+----------+------+--------+--------------+---------+\n",
      "|2022-02-10|  AMZN|159.0035|      159.0035| 68268000|\n",
      "|2022-02-11|  AMZN|153.2935|      153.2935| 77100000|\n",
      "|2022-02-14|  AMZN| 155.167|       155.167| 83230000|\n",
      "|2022-02-15|  AMZN|156.5105|      156.5105| 56440000|\n",
      "|2022-02-16|  AMZN|158.1005|      158.1005| 52704000|\n",
      "|2022-02-17|  AMZN|154.6525|      154.6525| 64032000|\n",
      "|2022-02-18|  AMZN|152.6015|      152.6015| 63604000|\n",
      "|2022-02-22|  AMZN|150.1975|      150.1975| 66128000|\n",
      "|2022-02-23|  AMZN| 144.827|       144.827| 64244000|\n",
      "|2022-02-24|  AMZN| 151.358|       151.358|100786000|\n",
      "|2022-02-25|  AMZN|153.7885|      153.7885| 62396000|\n",
      "|2022-02-28|  AMZN| 153.563|       153.563| 57684000|\n",
      "|2022-03-01|  AMZN| 151.142|       151.142| 44874000|\n",
      "|2022-03-02|  AMZN|152.0525|      152.0525| 47334000|\n",
      "|2022-03-03|  AMZN|147.8985|      147.8985| 65198000|\n",
      "|2022-03-04|  AMZN| 145.641|       145.641| 60934000|\n",
      "|2022-03-07|  AMZN| 137.453|       137.453| 86934000|\n",
      "|2022-03-08|  AMZN|136.0145|      136.0145| 91662000|\n",
      "|2022-03-09|  AMZN| 139.279|       139.279| 82656000|\n",
      "|2022-03-10|  AMZN|146.8175|      146.8175|135062000|\n",
      "+----------+------+--------+--------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "\n",
    "# register stock dataFrame as temp view with name \"stocks\"\n",
    "df_stx.createOrReplaceTempView(\"stocks\")\n",
    "\n",
    "# query the view\n",
    "sqlDF = spark.sql(\"SELECT * FROM stocks where ticker == 'AMZN'\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Given the stock dataframe, do an aggregation to compute minimum, mean, and maximum adjusted close for each stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-------------------+-------------------+\n",
      "|ticker|min(adjusted_close)|avg(adjusted_close)|max(adjusted_close)|\n",
      "+------+-------------------+-------------------+-------------------+\n",
      "|  AMZN|              81.82| 119.80421137714767|            169.315|\n",
      "|  MSFT|           213.6479|   260.898094693978|           313.1281|\n",
      "+------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stx.groupBy(\"ticker\").agg(F.min(\"adjusted_close\"), F.mean(\"adjusted_close\"), F.max(\"adjusted_close\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Select the date, ticker, and adjusted_close columns, saving this data as a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_stx.select(\"date\",\"ticker\",\"adjusted_close\").write.save(\"stocks.parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Load the parquet file into a new dataframe and verify that things look correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+\n",
      "|      date|ticker|adjusted_close|\n",
      "+----------+------+--------------+\n",
      "|2022-02-10|  MSFT|     299.57297|\n",
      "|2022-02-11|  MSFT|     292.30112|\n",
      "|2022-02-14|  MSFT|      292.2615|\n",
      "|2022-02-15|  MSFT|      297.6807|\n",
      "|2022-02-16|  MSFT|     297.33322|\n",
      "|2022-02-17|  MSFT|     288.62668|\n",
      "|2022-02-18|  MSFT|     285.84692|\n",
      "|2022-02-22|  MSFT|     285.63846|\n",
      "|2022-02-23|  MSFT|     278.24234|\n",
      "|2022-02-24|  MSFT|     292.45874|\n",
      "|2022-02-25|  MSFT|     295.15906|\n",
      "|2022-02-28|  MSFT|     296.62836|\n",
      "|2022-03-01|  MSFT|     292.81613|\n",
      "|2022-03-02|  MSFT|     298.01822|\n",
      "|2022-03-03|  MSFT|     293.77914|\n",
      "|2022-03-04|  MSFT|     287.76294|\n",
      "|2022-03-07|  MSFT|     276.89218|\n",
      "|2022-03-08|  MSFT|      273.8543|\n",
      "|2022-03-09|  MSFT|      286.4128|\n",
      "|2022-03-10|  MSFT|     283.52383|\n",
      "+----------+------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = spark.read.load(\"stocks.parquet\")\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 11. Summary\n",
    "\n",
    "You should now have a basic understanding of:\n",
    "\n",
    "- Spark SQL\n",
    "- DataFrames\n",
    "- When to use DataFrames vs RDDs\n",
    "- How to use some of the common transformations on DataFrames  \n",
    "\n",
    "Given practice, you will gain comfort in selecting and processing data with Spark SQL and DataFrames, which is essential.\n",
    "\n",
    "Additionally, you should have some sense of when DataFrames are preferred over RDDs, and vice versa.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
