{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uva_seal.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "### University of Virginia\n",
    "### DS 5110: Big Data Systems\n",
    "### Last Updated: March 15, 2022\n",
    "\n",
    "---  \n",
    "\n",
    "\n",
    "### SOURCES\n",
    "- Learning Spark, Chapter 11: Machine Learning with MLlib  \n",
    "- https://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html  \n",
    "- https://github.com/apache/spark/blob/master/examples/src/main/python/mllib/pca_rowmatrix_example.py  \n",
    "- https://github.com/apache/spark/blob/master/examples/src/main/python/mllib/svd_example.py  \n",
    "\n",
    "\n",
    "### OBJECTIVES\n",
    "-  Discuss purpose of dimensionality reduction  \n",
    "-  Introduce Principal Component Analysis $(PCA)$  \n",
    "-  Introduce Singular Value Decomposition $(SVD)$  \n",
    "\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Matrix rank  \n",
    "- Low-rank approximation of a matrix  \n",
    "- Dimensionality reduction  \n",
    "- $PCA$  \n",
    "- $SVD$  \n",
    "\n",
    "---\n",
    "\n",
    "**Rank of a Matrix**\n",
    "\n",
    "The **rank** of a matrix $A$ is the dimension of the vector space generated (or spanned) by its columns.  This corresponds to the maximal number of linearly independent columns of $A$.\n",
    "\n",
    "The **column rank** of $A$ is the dimension of the column space of $A$.  \n",
    "\n",
    "The **row rank** of $A$ is the dimension of the row space of $A$.  \n",
    "\n",
    "A fundamental result in linear algebra is that the column rank and the row rank are always equal.\n",
    "\n",
    "\n",
    "**Rank = 1**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\mathbf{A} =  \\begin{bmatrix}\n",
    "1 & 1 & 0 & 2  \\\\\n",
    "-1 & -1 & 0 & -2  \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rank = 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\mathbf{A} =  \\begin{bmatrix}\n",
    "1 & 0 & 1  \\\\\n",
    "-2 & -3 & 1  \\\\\n",
    "3 & 3 & 0  \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dimensionality Reduction** \n",
    "\n",
    "For a matrix of data with (independent) features along the columns, the dimension is equal to the number of features under consideration.  \n",
    "\n",
    "There are several reasons for reducing the number of dimensions including:  \n",
    "\n",
    "\n",
    "- Visualization  \n",
    "\n",
    "\n",
    "- $p >> n$ problem, or the curse of dimensionality (more features than observations)  \n",
    "\tInsufficient degrees of freedom to estimate a model\n",
    "\n",
    "\n",
    "- Saving on storage requirements  \n",
    "For example, a full matrix inversion step in regression (below) can be prohibitively large to store.\n",
    "\n",
    "\n",
    "$$ \\hat{β} = (X^TX)^{-1}X^TY $$\n",
    "\n",
    "\n",
    "Instead, the Gram matrix $X^{T}X$ can be replaced by a lower rank matrix decomposition from $SVD$, as an example.\n",
    "\n",
    "**Denoising the data** \n",
    "\n",
    "Even randomly generated data will produce a correlation matrix with extreme values by chance.  \n",
    "\n",
    "Compressing information into a smaller set of features will reduce noise  \n",
    "\n",
    "This is particularly useful when the covariance matrix is important \n",
    "(e.g., Mean Variance Portfolio Optimization in *Quantitative Finance*)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)**  \n",
    "\n",
    "$PCA$ is the primary technique used for dimensionality reduction  \n",
    "\n",
    "At a high level, $PCA$ constructs a set of new vectors which are a linear combination of the original vectors.  These new vectors have two special properties:  \n",
    "\n",
    "1. The vectors form an orthogonal basis, which means they are uncorrelated  \n",
    "2. The first principal component has the largest variance (it accounts for the most variability in the data), the second components has the next largest variance (while also being orthogonal to the first component), and so on.\n",
    "\n",
    "\n",
    "**PCA Illustrated in Two Dimensions**  \n",
    "The principal components point in the directions of greatest variance.  They are also orthogonal.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pca_img.gif\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of principal components will be equal to the starting number of vectors. However, only a subset of the components will be used (the top $k$).  \n",
    "\n",
    "**The top principal components can be used to create orthogonal, condensed features in a model.**  \n",
    "\n",
    "**Pitfalls of PCA**  \n",
    "$PCA$ includes a step where it computes the covariance matrix $X^{T}X$   \n",
    "This can lead to numerical rounding errors when calculating the eigenvalues/vectors.  \n",
    "\n",
    "For more details, please refer here for example:  \n",
    "http://en.wikipedia.org/wiki/Principal_component_analysis  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA Example (using RDD API)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 12:19:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "rows = sc.parallelize([\n",
    "    Vectors.sparse(5, {1: 1.0, 3: 7.0}),\n",
    "    Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n",
    "    Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/20 12:19:29 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "23/06/20 12:19:29 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    }
   ],
   "source": [
    "mat = RowMatrix(rows)\n",
    "\n",
    "# Compute the top 4 principal components.\n",
    "# Principal components are stored in a local dense matrix.\n",
    "pc = mat.computePrincipalComponents(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of mat: 3 rows, 5 columns\n"
     ]
    }
   ],
   "source": [
    "print('dimensions of mat: {} rows, {} columns'.format(mat.numRows(), mat.numCols()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of pc: 5 rows, 4 columns\n"
     ]
    }
   ],
   "source": [
    "print('dimensions of pc: {} rows, {} columns'.format(pc.numRows, pc.numCols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projected Row Matrix of principal component:\n",
      "[1.6485728230883814,-4.0132827005162985,-1.0091435193998504,-5.250587616986039]\n",
      "[-4.645104331781532,-1.1167972663619048,-1.0091435193998501,-5.250587616986039]\n",
      "[-6.428880535676488,-5.337951427775359,-1.009143519399851,-5.250587616986039]\n"
     ]
    }
   ],
   "source": [
    "# Project the rows to the linear space spanned by the top 4 principal components.\n",
    "projected = mat.multiply(pc)\n",
    "# Collect and print results\n",
    "collected = projected.rows.collect()\n",
    "print(\"Projected Row Matrix of principal component:\")\n",
    "for v in collected:\n",
    "  print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aside: can convert sparse vector to dense vector as follows:\n",
    "vs = Vectors.sparse(5, {1: 1.0, 3: 7.0})  # the sparse vector\n",
    "vd = Vectors.dense(vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA Example (using DataFrames API)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|features             |\n",
      "+---------------------+\n",
      "|(5,[1,3],[1.0,7.0])  |\n",
      "|[2.0,0.0,3.0,4.0,5.0]|\n",
      "|[4.0,0.0,0.0,6.0,7.0]|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors as dfVectors\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# set up some data\n",
    "data = [(dfVectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "        (dfVectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "        (dfVectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice the import line uses an alias dfVectors:**\n",
    "\n",
    "`from pyspark.ml.linalg import Vectors as dfVectors`\n",
    "\n",
    "We do this to avoid namespace collision with\n",
    "\n",
    "`from pyspark.mllib.linalg import Vectors`\n",
    "\n",
    "as both RDD and DF APIs have object *Vectors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------------------------------------------------------------------------+\n",
      "|features             |pcaFeatures                                                                    |\n",
      "+---------------------+-------------------------------------------------------------------------------+\n",
      "|(5,[1,3],[1.0,7.0])  |[1.6485728230883814,-4.0132827005162985,-1.0091435193998504,-5.250587616986039]|\n",
      "|[2.0,0.0,3.0,4.0,5.0]|[-4.645104331781533,-1.1167972663619048,-1.0091435193998504,-5.250587616986039]|\n",
      "|[4.0,0.0,0.0,6.0,7.0]|[-6.428880535676488,-5.337951427775359,-1.0091435193998508,-5.250587616986039] |\n",
      "+---------------------+-------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PCA using 4 components\n",
    "pca = PCA(k=4, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "\n",
    "model = pca.fit(df)\n",
    "\n",
    "# extract the transformed features\n",
    "result = model.transform(df)\n",
    "\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Singular Value Decomposition $(SVD)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$SVD$ is one of the major accomplishments of linear algebra, and it is a popular technique for factorizing an *m x n* (rectangular) matrix $A$ into three matrices with special properties.\n",
    "\n",
    "\\begin{equation*}\n",
    "A   = U \\Sigma V^T\n",
    "\\end{equation*}\n",
    "\n",
    "$U$ is an orthonormal *m x m* matrix; its columns are called *left singular vectors*  \n",
    "$\\Sigma$ is a rectangular *m x n* diagonal matrix. It has nonnegative values in descending order  \n",
    "$V$ is an orthonormal *n x n* matrix; its columns are called *right singular vectors*  \n",
    "\n",
    "The diagonal entries of $\\Sigma$ are known as the *singular values* of $A$.  \n",
    "They are the square roots of the eigenvalues from the matrix $AA^T$\n",
    "\n",
    "**<span style=\"color:red\">The purpose of $SVD$ is to select only the top *k* singular values and their associated singular vectors.  </span>**\n",
    "\n",
    "This means that we select only subsets of the matrices, which we denote as $\\hat{U}$ , $\\hat{\\Sigma}$, and $\\hat{V}^T$.  \n",
    "An approximation to  $A$  can then be constructed as \n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{A}   = \\hat{U} \\hat{\\Sigma} \\hat{V}^T\n",
    "\\end{equation*}\n",
    "\n",
    "$\\hat{U}$ has dimensions *m x k*  \n",
    "$\\hat{\\Sigma}$  has dimensions *k x k*  \n",
    "$\\hat{V}^T$ has dimensions *k x n*  \n",
    "\n",
    "The purpose of this factorization is to save on storage requirements, denoise, and recover the low-rank structure of the matrix.\n",
    "\n",
    "For more details, please refer here for example:  \n",
    "https://en.wikipedia.org/wiki/Singular_value_decomposition  \n",
    "\n",
    "- If $n$ is small $(n<100)$ or $k$ is large compared with $n (k > n/2)$, we compute the Gramian matrix first and then compute its top eigenvalues and eigenvectors locally on the driver. This requires a single pass with $O(n^2)$ storage on each executor, and $O(n^2k)$ time on the driver. \n",
    "\n",
    "\n",
    "- Otherwise, we compute $(A^TA)v$ in a distributed way and send it to $ARPACK$ to compute $(A^TA)$’s top eigenvalues and eigenvectors on the driver node. This requires $O(k)$ passes, $O(n)$ storage on each executor, and $O(nk)$ storage on the driver.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ARPACK**  \n",
    "$ARPACK$ is a collection of `Fortran77` subroutines designed to solve large scale eigenvalue problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$SVD$ Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U factor is:\n",
      "[-0.38829130511665644,-0.9198099362554474,-0.056387441301709175,9.313225746154785e-09]\n",
      "[-0.5301719995198351,0.2730185511901228,-0.8027319114319463,0.0]\n",
      "[-0.7537556058139434,0.2817987790459642,0.5936682026454339,1.4901161193847656e-08]\n",
      "Singular values are: [13.029275535600473,5.368578733451684,2.5330498218813755,6.323166049206486e-08]\n",
      "V factor is:\n",
      "DenseMatrix([[-0.31278534,  0.31167136,  0.30366911,  0.8409913 ],\n",
      "             [-0.02980145, -0.17133211, -0.02226069,  0.14664984],\n",
      "             [-0.12207248,  0.15256471, -0.95070998,  0.23828799],\n",
      "             [-0.71847899, -0.68096285, -0.0172245 , -0.02094998],\n",
      "             [-0.60841059,  0.62170723,  0.05606596, -0.46260933]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "rows = sc.parallelize([\n",
    "    Vectors.sparse(5, {1: 1.0, 3: 7.0}),\n",
    "    Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n",
    "    Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)\n",
    "])\n",
    "\n",
    "mat = RowMatrix(rows)\n",
    "\n",
    "# Compute the top k singular values and corresponding singular vectors.\n",
    "topk = 4\n",
    "svd = mat.computeSVD(topk, computeU=True)\n",
    "U = svd.U       # The U factor is a RowMatrix.\n",
    "s = svd.s       # The singular values are stored in a local dense vector.\n",
    "V = svd.V       # The V factor is a local dense matrix.\n",
    "\n",
    "collected = U.rows.collect()\n",
    "print(\"U factor is:\")\n",
    "for vector in collected:\n",
    "    print(vector)\n",
    "print(\"Singular values are: %s\" % s)\n",
    "print(\"V factor is:\\n%s\" % V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY FOR YOURSELF (UNGRADED EXERCISES)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **PCA**  \n",
    "i. Copy the PCA example code into the cell below  \n",
    "ii. Define an RDD containing several vectors, each with length 5  \n",
    "iii. Compute the principal components  \n",
    "iv. Produce the scree plot and decide on the optimal number of components using the elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) **SVD**  \n",
    "i. Copy the SVD example code into the cell below  \n",
    "ii. Define an RDD using the same RDD from your PCA exercise  \n",
    "iii. For singular values $k= 2, 3, 4$:  \n",
    "- compute the approximation to the matrix  \n",
    "- compute the Frobenius norm between the actual and approximate matrix, $||M_{act}-M_{approx}||_F$  \n",
    "This matrix norm is the square root of the sum of the absolute squared element-wise differences between the matrices.  It measures a distance between two matrices.  \n",
    "\n",
    "Hint: you may want to the convert data to numpy arrays.  Converting from RDD to numpy arrays can be done like this:\n",
    "\n",
    "a = np.array(RDD.collect())\n",
    "\n",
    "iv. What do you notice about the norm as $k$ increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DS5110 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
