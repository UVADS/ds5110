{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./uva_seal.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Hotspot Demo\n",
    "\n",
    "### University of Virginia\n",
    "### DS 5110: Big Data Systems\n",
    "### Last Updated: January 19, 2026\n",
    "\n",
    "---  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BACKGROUND\n",
    "\n",
    "In Spark, a data hotspot happens when one or a few keys have much more data than others — causing one task to do most of the work while others sit idle.\n",
    "\n",
    "It is a data imbalance problem.\n",
    "\n",
    "This notebook demonstrates two small examples of how it arises and how it can be remediated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXAMPLE 1: SALTING A SMALLER DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"HotspotDemo\").getOrCreate()\n",
    "\n",
    "# Create skewed data\n",
    "data = [\n",
    "    (\"user1\", \"click\"), (\"user1\", \"click\"), (\"user1\", \"click\"), (\"user1\", \"click\"), (\"user1\", \"click\"),\n",
    "    (\"user1\", \"click\"), (\"user1\", \"click\"), (\"user1\", \"click\"), (\"user1\", \"click\"), (\"user1\", \"click\"), # Hot key\n",
    "    (\"user2\", \"click\"), (\"user3\", \"click\"), (\"user4\", \"click\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"user_id\", \"event\"])\n",
    "\n",
    "# Aggregate by user_id which causes data skew on \"user1\"\n",
    "counts = df.groupBy(\"user_id\").agg(count(\"*\").alias(\"total_events\"))\n",
    "\n",
    "counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### THE PROBLEM\n",
    "\n",
    "Most records belong to \"user1\" key.\n",
    "\n",
    "During `groupBy`, that key becomes a hot partition.\n",
    "\n",
    "Spark assigns a single task to handle all \"user1\" data, slowing the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A SOLUTION: SALTING\n",
    "\n",
    "Have Spark add a small random “salt” (0–9) to each record’s key, spreading the hot key’s data across multiple reducers.\n",
    "\n",
    "Then aggregate back to the original key after processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, concat, rand, expr\n",
    "\n",
    "# Add a random salt to break up the skewed key\n",
    "salted = df.withColumn(\"salted_key\", concat(col(\"user_id\"), lit(\"_\"), (rand() * 10).cast(\"int\")))\n",
    "print('Salted data:')\n",
    "salted.show()\n",
    "\n",
    "# Aggregate by salted key\n",
    "salted_counts = salted.groupBy(\"salted_key\").agg(count(\"*\").alias(\"partial_count\"))\n",
    "print('Salted counts by key:')\n",
    "salted_counts.show()\n",
    "\n",
    "# Aggregate back by original user_id\n",
    "final_counts = salted_counts.groupBy(expr(\"split(salted_key, '_')[0]\").alias(\"user_id\")) \\\n",
    "                            .agg(expr(\"sum(partial_count)\").alias(\"total_events\"))\n",
    "\n",
    "print('Counts by original key:')\n",
    "final_counts.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### EXAMPLE 2: LARGER EXAMPLE OF SALTING WITH RUNTIME COMPARE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a large, skewed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "import time\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"HotspotTimingDemo\").getOrCreate()\n",
    "\n",
    "# 99% of rows belong to one key (\"hot_user\")\n",
    "data = [(\"hot_user\", i) for i in range(990000)] + \\\n",
    "       [(f\"user_{i}\", i) for i in range(1, 10000)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"user_id\", \"event_id\"])\n",
    "\n",
    "print(f\"Total rows: {df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect some rows\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, we aggregate without fixing skew and compute runtime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "counts_no_fix = df.groupBy(\"user_id\").agg(count(\"*\").alias(\"total_events\"))\n",
    "counts_no_fix.count()  # force computation\n",
    "time_no_fix = time.time() - start_time\n",
    "print(f\"Runtime without fix: {time_no_fix:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salt the data to break up skew**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salted = df.withColumn(\"salted_key\", concat(col(\"user_id\"), lit(\"_\"), (rand() * 10).cast(\"int\")))\n",
    "\n",
    "salted_counts = salted.groupBy(\"salted_key\").agg(count(\"*\").alias(\"partial_count\"))\n",
    "\n",
    "final_counts = salted_counts.groupBy(expr(\"split(salted_key, '_')[0]\").alias(\"user_id\")) \\\n",
    "                            .agg(expr(\"sum(partial_count)\").alias(\"total_events\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute and compare runtimes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "final_counts.count()  # force computation\n",
    "time_with_fix = time.time() - start_time\n",
    "print(f\"Runtime with salting fix: {time_with_fix:.2f} seconds\")\n",
    "\n",
    "# compare runtimes\n",
    "improvement = ((time_no_fix - time_with_fix) / time_no_fix) * 100\n",
    "print(f\"Improvement: {improvement:.1f}% faster after removing hotspot\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 4.0.0",
   "language": "python",
   "name": "pyspark-4.0.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
