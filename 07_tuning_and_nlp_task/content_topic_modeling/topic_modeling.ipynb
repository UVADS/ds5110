{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"uva_seal.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling in Spark\n",
    "Source: https://medium.com/analytics-vidhya/distributed-topic-modelling-using-spark-nlp-and-spark-mllib-lda-6db3f06a4da3\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "In this notebook, we will walk through a Topic Modeling task.  \n",
    "The purpose of this exercise is to: \n",
    "\n",
    "- Get you comfortable working with a repo and some external code\n",
    "- Check your understanding of Spark / Show you how much you've learned about Spark!\n",
    "\n",
    "It is not important to know Topic Modeling for the exercise, but you can read about it [here](http://www.cs.columbia.edu/~blei/papers/Blei2011.pdf)\n",
    "\n",
    "#### Getting the data\n",
    "\n",
    "The source article (above) provides a link to the dataset.  \n",
    "In this folder, the file: `topic_modeling_repo_setup.png` shows the steps in terminal to get the data from the repo.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install the Spark NLP package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spark-nlp==4.3.2 in /home/apt4c/.local/lib/python3.7/site-packages (4.3.2)\n"
     ]
    }
   ],
   "source": [
    "# This version will be compatible with Spark 3.3\n",
    "! pip install spark-nlp==4.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import sparknlp\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/apt4c/.ivy2/cache\n",
      "The jars for the packages stored in: /home/apt4c/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3977000e-8284-4ccf-bb3d-0f838eb2438e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;4.3.2 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.16.0 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.16 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.42.3 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.42.3 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.42.3 in central\n",
      "\tfound com.google.api-client#google-api-client;2.1.1 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.42.3 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.9.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.9.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.42.3 in central\n",
      "\tfound com.google.api#gax-httpjson;0.105.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.9.0 in central\n",
      "\tfound io.grpc#grpc-core;1.51.0 in central\n",
      "\tfound com.google.api#gax;2.20.1 in central\n",
      "\tfound com.google.api#gax-grpc;2.20.1 in central\n",
      "\tfound io.grpc#grpc-alts;1.51.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.51.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.51.0 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.13.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.13.0 in central\n",
      "\tfound com.google.api#api-common;2.2.2 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound io.grpc#grpc-context;1.51.0 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.6.22 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.10 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.10 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.11.0 in central\n",
      "\tfound org.threeten#threetenbp;1.6.4 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.16.0-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.16.0-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.16.0-alpha in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.14.1 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.51.0 in central\n",
      "\tfound io.grpc#grpc-auth;1.51.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.51.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.28.0 in central\n",
      "\tfound com.google.api.grpc#grpc-google-iam-v1;1.6.22 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.51.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.51.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.51.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.51.0 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.51.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      ":: resolution report :: resolve 2409ms :: artifacts dl 100ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.14.1 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.2.2 from central in [default]\n",
      "\tcom.google.api#gax;2.20.1 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.20.1 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.105.1 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.1.1 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.16.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.16.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-iam-v1;1.6.22 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.16.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.11.0 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.6.22 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.13.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.13.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.9.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.9.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.9.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.16.0 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.16 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.42.3 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.10 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.10 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;4.3.2 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.51.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.51.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.28.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.4 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.10] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.10] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   73  |   0   |   0   |   3   ||   70  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3977000e-8284-4ccf-bb3d-0f838eb2438e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 70 already retrieved (0kB/64ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/19 01:53:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\") \\\n",
    "    .config(\"spark.driver.memory\",\"8G\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.3.2\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1000M\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1041793"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_data = '/sfs/qumulo/qhome/apt4c/topic_modeling/data/abcnews-date-text.csv'\n",
    "file_type = \"csv\"\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(path_to_data)\n",
    "# Verify the count\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the first few records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------------------------------------+\n",
      "|publish_date|headline_text                                     |\n",
      "+------------+--------------------------------------------------+\n",
      "|20030219    |aba decides against community broadcasting licence|\n",
      "|20030219    |act fire witnesses must be aware of defamation    |\n",
      "|20030219    |a g calls for infrastructure protection summit    |\n",
      "|20030219    |air nz staff in aust strike for pay rise          |\n",
      "|20030219    |air nz strike to affect australian travellers     |\n",
      "+------------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark NLP requires the input dataframe or column to be converted to document. \n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"headline_text\") \\\n",
    "    .setOutputCol(\"document\") \\\n",
    "    .setCleanupMode(\"shrink\")\n",
    "\n",
    "# Split sentence to tokens(array)\n",
    "tokenizer = Tokenizer() \\\n",
    "  .setInputCols([\"document\"]) \\\n",
    "  .setOutputCol(\"token\")\n",
    "\n",
    "# Clean unwanted characters\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\")\n",
    "\n",
    "# remove stopwords\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "      .setInputCols(\"normalized\")\\\n",
    "      .setOutputCol(\"cleanTokens\")\\\n",
    "      .setCaseSensitive(False)\n",
    "\n",
    "# Stem the words to bring them to the root form.\n",
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"cleanTokens\"]) \\\n",
    "    .setOutputCol(\"stem\")\n",
    "\n",
    "# Finisher is the most important annotator. \n",
    "# Spark NLP adds its own structure when we convert each row in the dataframe to document. \n",
    "# Finisher helps us to bring back the expected structure viz. array of tokens.\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"stem\"]) \\\n",
    "    .setOutputCols([\"tokens\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(False)\n",
    "\n",
    "# Build the ML Pipeline. Many of these steps are common in NLP.\n",
    "nlp_pipeline = Pipeline(\n",
    "    stages=[document_assembler, \n",
    "            tokenizer,\n",
    "            normalizer,\n",
    "            stopwords_cleaner, \n",
    "            stemmer, \n",
    "            finisher])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/opt/conda/lib/python3.7/site-packages/pyspark/jars/spark-core_2.12-3.3.1.jar) to field java.util.regex.Pattern.pattern\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    }
   ],
   "source": [
    "nlp_model = nlp_pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the pipeline to transform dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = nlp_model.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the columns that we need, and first 10K records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|publish_date|              tokens|\n",
      "+------------+--------------------+\n",
      "|    20030219|[aba, decid, comm...|\n",
      "|    20030219|[act, fire, wit, ...|\n",
      "|    20030219|[g, call, infrast...|\n",
      "|    20030219|[air, nz, staff, ...|\n",
      "|    20030219|[air, nz, strike,...|\n",
      "|    20030219|[ambiti, olsson, ...|\n",
      "|    20030219|[antic, delight, ...|\n",
      "|    20030219|[aussi, qualifi, ...|\n",
      "|    20030219|[aust, address, u...|\n",
      "|    20030219|[australia, lock,...|\n",
      "|    20030219|[australia, contr...|\n",
      "|    20030219|[barca, take, rec...|\n",
      "|    20030219|[bathhous, plan, ...|\n",
      "|    20030219|[big, hope, launc...|\n",
      "|    20030219|[big, plan, boost...|\n",
      "|    20030219|[blizzard, buri, ...|\n",
      "|    20030219|[brigadi, dismiss...|\n",
      "|    20030219|[british, combat,...|\n",
      "|    20030219|[bryant, lead, la...|\n",
      "|    20030219|[bushfir, victim,...|\n",
      "+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokens_df = processed_df.select('publish_date','tokens').limit(10000)\n",
    "tokens_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a vocabulary of 500 tokens and create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\", vocabSize=500, minDF=3.0)\n",
    "\n",
    "# train the model\n",
    "cv_model = cv.fit(tokens_df)\n",
    "\n",
    "# transform the data. Output column name will be features.\n",
    "vectorized_tokens = cv_model.transform(tokens_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at some records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------------------------------+---------------------------------------------------------------+\n",
      "|publish_date|tokens                                       |features                                                       |\n",
      "+------------+---------------------------------------------+---------------------------------------------------------------+\n",
      "|20030219    |[aba, decid, commun, broadcast, licenc]      |(500,[118,498],[1.0,1.0])                                      |\n",
      "|20030219    |[act, fire, wit, must, awar, defam]          |(500,[12,116,389],[1.0,1.0,1.0])                               |\n",
      "|20030219    |[g, call, infrastructur, protect, summit]    |(500,[14,444],[1.0,1.0])                                       |\n",
      "|20030219    |[air, nz, staff, aust, strike, pai, rise]    |(500,[59,61,112,117,152,292,475],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|20030219    |[air, nz, strike, affect, australian, travel]|(500,[61,93,112,292],[1.0,1.0,1.0,1.0])                        |\n",
      "+------------+---------------------------------------------+---------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vectorized_tokens.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Latent Dirichlet Allocation (LDA) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -178739.95728641868\n",
      "The upper bound on perplexity: 6.299871608854458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "num_topics = 3\n",
    "lda = LDA(k=num_topics, maxIter=10)\n",
    "\n",
    "model = lda.fit(vectorized_tokens)\n",
    "ll = model.logLikelihood(vectorized_tokens)\n",
    "lp = model.logPerplexity(vectorized_tokens)\n",
    "\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "win\n",
      "man\n",
      "charg\n",
      "polic\n",
      "face\n",
      "council\n",
      "world\n",
      "urg\n",
      "court\n",
      "get\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "u\n",
      "iraq\n",
      "war\n",
      "new\n",
      "iraqi\n",
      "baghdad\n",
      "plan\n",
      "sai\n",
      "protest\n",
      "call\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "govt\n",
      "nsw\n",
      "set\n",
      "crash\n",
      "deni\n",
      "kill\n",
      "claim\n",
      "rain\n",
      "troop\n",
      "sai\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "# extract vocabulary from CountVectorizer\n",
    "vocab = cv_model.vocabulary\n",
    "topics = model.describeTopics()   \n",
    "topics_rdd = topics.rdd\n",
    "\n",
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(f\"topic: {idx}\")\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS5110 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
