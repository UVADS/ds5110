{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uva_seal.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL and DataFrames\n",
    "\n",
    "### University of Virginia\n",
    "### DS 5110: Big Data Systems\n",
    "### Last Updated: Feb 23, 2021\n",
    "\n",
    "---  \n",
    "\n",
    "### Sources \n",
    "\n",
    "Learning Spark, Chapter 9: Spark SQL\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning\n",
    "\n",
    "Demonstration of several useful DataFrame operations:  \n",
    "https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html\n",
    "\n",
    "### OBJECTIVES\n",
    "- Introduction to Spark SQL, the interface for working with structured and semistructured data\n",
    "- Introduce DataFrames and show basic functionality\n",
    "- Discuss SparkSession\n",
    "\n",
    "### CONCEPTS AND FUNCTIONS\n",
    "- Schema\n",
    "- SQL\n",
    "- Dataset and DataFrame\n",
    "- Partition\n",
    "- Parquet files\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTES**\n",
    "\n",
    "These lecture notes are a quick outline of Spark SQL and DataFrames.  \n",
    "There is a lot of functionality provided, and Spark SQL is a heavy development area.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema\n",
    "\n",
    "A database *schema* is the structure that represents the logical view of the entire database.  It defines how data is organized and how relations among them are associated.  This is implemented through the use of tables, views, and integrity constraints.\n",
    "\n",
    "### Schema in Spark\n",
    "\n",
    "The schema in Spark defines the data structure. For each field, a 3-tuple is specified: `(column name, data type, nullable)`  \n",
    "\n",
    "---  \n",
    "\n",
    "**Example of schema with two Fields *author* and *pages*, which cannot contain null values**\n",
    "```\n",
    "schema = StructType([StructField(\"author\", StringType(), False), StructField(\"pages\", IntegerType(), False)])\n",
    "```\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to allow Spark to infer the schema of your data, but it's preferable to feed it the schema:\n",
    "\n",
    "- avoids having Spark launch a separate job to read a large fraction of the data to infer schema\n",
    "- early detection of errors if the data doesn't match the schema\n",
    "- Spark inference may be incorrect. For example, it may think all numerical data are strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Spark Data Types\n",
    "\n",
    "- integer types, all `int` in python:\n",
    "  - ShortType\n",
    "  - IntegerType\n",
    "  - LongType\n",
    "  - FloatType\n",
    "  - DoubleType\n",
    "- StringType\n",
    "- BooleanType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL in Ten Seconds (tongue in cheek)\n",
    "\n",
    "\n",
    "SQL is a structured query language used to communicate with relational databases.  \n",
    "Commands include CREATE, SELECT, UPDATE, ALTER, INSERT INTO, DROP, DELETE.  \n",
    "This course will use SELECT.\n",
    "\n",
    "### Spark SQL Capabilities:\n",
    "\n",
    "- load data from various structured formats including JSON, Hive, Parquet  \n",
    "- query data using SQL inside Spark or from external tools that connect to Spark (e.g., `Tableau`) \n",
    "- Spark SQL integrates between SQL and Python/Java/Scala/R code. Can do things like join RDDs and SQL tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and DataFrame\n",
    "\n",
    "- A Dataset is a distributed collection of data   \n",
    "- A Dataset can be constructed from JVM objects and then manipulated using functional transformations (`map()`, `flatMap()`, `filter()`, etc.)  \n",
    "- A DataFrame is a Dataset organized into named columns   \n",
    "\n",
    "In practice, you will be thinking in terms of `DataFrames`, and not `Datasets`.  For users familiar with dataframes from R and Python, they are similar, yet with operations distinct to Spark.  As an example, adding a new column to a DataFrame is executed using `withColumn()`.  This may feel more formal compared to R and Python.  \n",
    "\n",
    "Additionally - when compared to R and Python - the Spark DataFrame uses richer optimizations under the hood.  The structure makes use of distributed computing, in the same manner as RDDs.  \n",
    "\n",
    "DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.  \n",
    "\n",
    "The DataFrame API is available in Scala, Java, Python, and R. \n",
    "\n",
    "### DataFrames vs RDDs  \n",
    "\n",
    "Now that we have two powerful objects that parallelize data, we have more flexibility, but this can lead to confusion.  When is it better to use DataFrames, and when is it better to use RDDs?  \n",
    "\n",
    "Here are some recommendations:   \n",
    "\n",
    "- In general, most work can be done with DataFrames  \n",
    "\n",
    "- Use DataFrames to use high-level expressions, to perform SQL queries to explore the data, and to gain columnar access.  For example, if you are thinking about the data by field names, you probably want the data in a DataFrame.\n",
    "\n",
    "- For machine learning and building predictive models, DataFrames are recommended. You will be exploring the data by column, and building features from the columns of data.  \n",
    "- RDDs can be useful to perform low-level transformations and actions on unstructured data. For example, filtering strings and performing other simple transformations on text is best done with RDDs.  In these cases, the analyst doesn't care about field names, and there is no need to impose schema on the data.  \n",
    "\n",
    "- Use RDDs when you want to manipulate the data with functional programming constructs rather than domain specific expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DataFrame\n",
    "\n",
    "There are multiple ways to do this:\n",
    "- use a function such as `read.csv()` to read data from files into DataFrames (most common)\n",
    "- pass data to `createDataFrame()`\n",
    "- conversion from RDD using `toDF()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1: Create DataFrame from RDD using `toDF()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "```\n",
    "# import modules \n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Map the RDD to a DF\n",
    "\n",
    "df = rdd.map(lambda line: Row(longitude=line[0], \n",
    "                              latitude=line[1], \n",
    "                              housingMedianAge=line[2],\n",
    "                              totalRooms=line[3],\n",
    "                              totalBedRooms=line[4],\n",
    "                              population=line[5], \n",
    "                              households=line[6],\n",
    "                              medianIncome=line[7],\n",
    "                              medianHouseValue=line[8])).toDF()\n",
    "```\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2: Create DataFrame by passing data and schema to `createDataFrame()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  0|Hi I heard about ...|\n",
      "|  1|I wish Java could...|\n",
      "|  2|Logistic regressi...|\n",
      "+---+--------------------+\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# import context manager: SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# import data types\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# set up the session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# create some data; list of tuples\n",
    "data = [\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I wish Java could use case classes\"),\n",
    "    (2, \"Logistic regression models are neat\")\n",
    "]\n",
    "\n",
    "# define schema; each field holds (name, data type, nullable)\n",
    "# for large number of fields, best to automate schema construction\n",
    "schema = StructType([StructField('id', IntegerType(), False), \n",
    "                     StructField('sentence', StringType(), False)])\n",
    "\n",
    "# create df by passing data, schema\n",
    "sentenceDataFrame = spark.createDataFrame(data, schema)\n",
    "\n",
    "# print first few records\n",
    "sentenceDataFrame.show()\n",
    "\n",
    "# print data type\n",
    "print(type(sentenceDataFrame))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3: Create a DataFrame from some JSON data**  \n",
    "(For an example of JSON data see: http://json.org/example.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Read data in json format\n",
    "df = spark.read.json(\"people.json\")\n",
    "\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an RDD from a DataFrame\n",
    "\n",
    "This is very simple: `df.rdd`\n",
    "\n",
    "Here we convert our df containing sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_rdd = sentenceDataFrame.rdd\n",
    "print(sentence_rdd.take(2))\n",
    "print(type(sentence_rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkSession\n",
    "\n",
    "The `SparkSession` is a unified conduit to all Spark operations and data.  It's an example of a `context manager`.  \n",
    "\n",
    "Spark used to use many context managers to the point of confusion.  \n",
    "From the developers:  \n",
    "\n",
    "*We have been getting a lot of questions about the relationship between SparkContext, SQLContext, and HiveContext in Spark 1.x. It was really strange to have “HiveContext” as an entry point when people want to use the DataFrame API. In Spark 2.0, we are introducing SparkSession, a new entry point that subsumes SQLContext and HiveContext. For backward compatibility, we keep the Hive and SQL Contexts.*  \n",
    "\n",
    "For details:  \n",
    "https://docs.databricks.com/spark/latest/gentle-introduction/sparksession.html\n",
    "\n",
    "Here is an example of building a more elaborate SparkSession:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SparkSession Example Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\                         # use all cores on local machine\n",
    "    .appName(\"Python Spark SQL basic example\") \\  # will see appName on cluster manager\n",
    "    .config(\"spark.executor.memory\", '20g') \\     # RAM per executor (worker)\n",
    "    .config('spark.executor.cores', '5') \\        # cores available to EACH executor\n",
    "    .config('spark.executor.instances', '17') \\   # total number of executors\n",
    "    .config(\"spark.driver.memory\",'1g') \\         # RAM for driver, generally lower need than a worker\n",
    "    .getOrCreate()\n",
    "    \n",
    "# for details see:\n",
    "# https://spark.apache.org/docs/latest/configuration.html\n",
    "```\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Cores, Executors, RAM \n",
    "\n",
    "NOTES  \n",
    "- setting these configs is best codified in a function\n",
    "- Spark sets configs by default, but unfortunately they're not always optimal\n",
    "\n",
    "---  \n",
    "\n",
    "<span style=\"color:red\">**Example: Hardware consists of 6 nodes, each with 16 cores, 64GB RAM**</span>\n",
    "\n",
    "RESOURCE OVERHEAD:  \n",
    "$O1$. On each executor, 1 core and 1 GB RAM is consumed by OS and Hadoop Daemons  \n",
    "This leaves 15 available cores on each node  \n",
    "$O2$. The resource manager (e.g., YARN) will require an overhead ~1GB RAM per node  \n",
    "$O3$. One executor is required for the driver\n",
    "\n",
    "**Number of cores**  \n",
    "More cores means more concurrent processing, but an application running > 5 concurrent tasks generally doesn't perform well.  \n",
    "cap this at **spark.executor.cores = 5**.  \n",
    "\n",
    "**Executor instances**  \n",
    "We can set 15 cores_per_node / 5 cores_per_executor = 3 executors_per_node. 15 is due to $O1$.\n",
    "\n",
    "Given 6 nodes and 3 executors per node, we can set 18 executors  \n",
    "One of these executors is required for the driver $(O3)$    \n",
    "Thus, we set **spark.executor.instances = 17**  \n",
    "\n",
    "**Executor memory**  \n",
    "Available RAM is 63GB per node $(O1)$. For 3 executors per node, this gives 63GB/3 = 21GB per executor  \n",
    "The resource manager will require an overhead ~1GB per node $(O2)$. set **spark.executor.memory = 20g**\n",
    "\n",
    "**NOTE:** `spark.executor.cores` will use all cores by default (this is a simpler way to go, but not always optimal)  \n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Useful Operations\n",
    "\n",
    "Next, we turn to the documentation to explore more DataFrame functionality including subsetting, filtering, aggregation.  \n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "There are several different ways to extract columns from a DataFrame, shown below with examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data in json format\n",
    "df = spark.read.json(\"people.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "Notes: \n",
    "- `col()` extracts a column from a DataFrame  \n",
    "- `asc()` takes an optional parameter to sort ascending or descending  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep records where age > 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep records subject to filters on name, then sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, asc\n",
    "\n",
    "df.filter((col(\"name\") == \"Andy\") | (col(\"name\") == \"Michael\")).sort(asc(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alternatively using df.name instead of col(\"name\")\n",
    "\n",
    "df.filter((df.name == \"Andy\") | (df.name == \"Michael\")).sort(asc(\"name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Fetch records with age *null*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"age\").isNull()).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch records with age *not null*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 30|  Andy|\n",
      "| 19|Justin|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"age\").isNotNull()).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### where() is equivalent to filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where((col(\"name\") == \"Andy\") | (col(\"name\") == \"Michael\")).sort(asc(\"name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing with 0 (just for illustration; not a great idea for this data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "|  0|Michael|\n",
      "| 30|   Andy|\n",
      "| 19| Justin|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the age field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               age|\n",
      "+-------+------------------+\n",
      "|  count|                 2|\n",
      "|   mean|              24.5|\n",
      "| stddev|7.7781745930520225|\n",
      "|    min|                19|\n",
      "|    max|                30|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe(\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL Queries\n",
    "\n",
    "To write SQL queries against DataFrames, first register as a `SQL temp view`, and then write the query.\n",
    "\n",
    "**Example of SQL Query against DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# register DataFrame as temp view with name \"people\"\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# query the view\n",
    "sqlDF = spark.sql(\"SELECT * FROM people where name == 'Andy'\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate on columns\n",
    "\n",
    "SQL functions can be loaded from this library: `pyspark.sql.functions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Group by the location column to compute the min, count, and avg\n",
    "\n",
    "---  \n",
    "```\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "agg_df = df.groupBy(\"location\").agg(F.min(\"id\"), F.count(\"id\"), F.avg(\"date_diff\"))\n",
    "```\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write DF to Parquet file, partitioning columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "```\n",
    "df = df.withColumn('end_month', F.month('end_date'))\n",
    "df = df.withColumn('end_year', F.year('end_date'))\n",
    "df.write.partitionBy(\"end_year\", \"end_month\").parquet(\"/tmp/sample_table\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer the schema when reading in file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "```\n",
    "adult_df = spark.read.\\\n",
    "    format(\"com.spark.csv\").\\\n",
    "    option(\"header\", \"false\").\\\n",
    "    option(\"inferSchema\", \"true\").load(\"dbfs:/databricks-datasets/adult/adult.data\")\n",
    "adult_df.printSchema()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save / Load using Generic Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "df = spark.read.load(\"examples/src/main/resources/users.parquet\")\n",
    "df.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save / Load using Manually Specified Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "df = spark.read.load(\"examples/src/main/resources/people.json\", format=\"json\")\n",
    "df.select(\"name\", \"age\").write.save(\"namesAndAges.parquet\", format=\"parquet\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet Files\n",
    "\n",
    "- Project was developed at Twitter, taken over by Apache Software Foundation (Apache)   \n",
    "- Parquet is a columnar format that is supported by many other data processing systems  \n",
    "\n",
    "- Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data. When writing Parquet files, all columns are automatically converted to be nullable for compatibility reasons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key observation: It can be much more efficient to store data in terms of columns than rows.  \n",
    "Column data is stored in contiguous memory blocks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save / Load Operations using Parquet Files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "```\n",
    "# read in data in JSON format. This will produce a DataFrame.\n",
    "peopleDF = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "\n",
    "# DataFrames can be saved as Parquet files, maintaining the schema information.\n",
    "peopleDF.write.parquet(\"people.parquet\")\n",
    "\n",
    "# Read in the Parquet file created above.\n",
    "# Parquet files are self-describing so the schema is preserved.\n",
    "# Loading parquet files produces a DataFrame.\n",
    "parquetFile = spark.read.parquet(\"people.parquet\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database tables can be partitioned to make querying more efficient.  \n",
    "For example, the data can be\n",
    "split by gender and country, producing smaller tables.  \n",
    "If the analyst is only interested in a single country, the query will run faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a partitioned table, data are usually stored in different directories, with partitioning column values encoded in the path of each partition directory.  \n",
    "\n",
    "All built-in file sources (including Text/CSV/JSON/ORC/Parquet) are able to discover and infer partitioning information automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path\n",
    "└── to\n",
    "    └── table\n",
    "        ├── gender=male\n",
    "        │   ├── ...\n",
    "        │   │\n",
    "        │   ├── country=US\n",
    "        │   │   └── data.parquet\n",
    "        │   ├── country=CN\n",
    "        │   │   └── data.parquet\n",
    "        │   └── ...\n",
    "        └── gender=female\n",
    "            ├── ...\n",
    "            │\n",
    "            ├── country=US\n",
    "            │   └── data.parquet\n",
    "            ├── country=CN\n",
    "            │   └── data.parquet\n",
    "            └── ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY FOR YOURSELF (UNGRADED EXERCISES)**\n",
    "\n",
    "Run the code below to create a DataFrame.  Then complete the tasks that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sentenceData = spark.createDataFrame([ \\\n",
    "    (0, \"the quick brown\"), \\\n",
    "    (1, \"fox jumped\") \\\n",
    "], [\"id\", \"raw\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Print `sentenceData`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Print the record count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Print the row containing the word \"fox\".  Do this by registering the dataframe as a SQL temp view, and then query it using `spark.sql`.  Hint: the `like` command will be helpful in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Print the row containing the word \"fox.\" Do this by using the functions `filter()` and `contains()`.  Note this method is probably simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**  \n",
    "You should now have a basic understanding of Spark SQL, DataFrames, and how to use some of the common transformations on DataFrames.  Additionally, you should have some sense of when DataFrames are preferred over RDDs, and vice versa."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
