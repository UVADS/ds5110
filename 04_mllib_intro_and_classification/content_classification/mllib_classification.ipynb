{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"uva_seal.png\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib Intro and Classifiers\n",
    "\n",
    "### University of Virginia\n",
    "### DS 5110: Big Data Systems\n",
    "### Last Updated: June 16, 2021\n",
    "\n",
    "---  \n",
    "\n",
    "\n",
    "### SOURCES \n",
    "Learning Spark, Chapter 11: Machine Learning with MLlib\n",
    "\n",
    "https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression\n",
    "\n",
    "https://spark.apache.org/docs/latest/mllib-naive-bayes.html\n",
    "\n",
    "http://spark.apache.org/docs/latest/mllib-decision-tree.html\n",
    "\n",
    "http://spark.apache.org/docs/latest/mllib-ensembles.html\n",
    "\n",
    "http://spark.apache.org/docs/latest/ml-classification-regression.html#linear-support-vector-machine\n",
    "\n",
    "https://spark.apache.org/docs/2.1.2/api/java/org/apache/spark/mllib/util/MLUtils.html\n",
    "\n",
    "Logistic Regression using the DataFrame API  \n",
    "https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### OBJECTIVES\n",
    "- Introduce the major classification methods in MLlib using the RDD API\n",
    "- Introduce a Logistic Regression model using the DataFrame API\n",
    " \n",
    "\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Supervised learning\n",
    "- Binary and multiclass classification\n",
    "- Logistic Regression\n",
    "- Naive Bayes\n",
    "- Tree methods\n",
    "- Decision Trees\n",
    "- Random Forests\n",
    "- Gradient-Boosted Trees\n",
    "- Ensemble\n",
    "- Support vector machines\n",
    "\n",
    "---\n",
    "\n",
    "**Game Plan**\n",
    "\n",
    "This notebook will do three things:\n",
    "\n",
    "1. Briefly summarize some of the major classification models\n",
    "2. Illustrate how to work with these models using the RDD interface\n",
    "3. Illustrate how to work with Logistic Regression using the DataFrame interface\n",
    "\n",
    "**NOTE**  \n",
    "The assignments will be explicit about which interface to use in various tasks.  \n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Machine Learning in Spark \n",
    "Spark MLlib is the library for machine learning.  There are two interfaces:\n",
    "\n",
    "1) A newer DataFrame-based API which is being actively built out\n",
    "\n",
    "2) An older RDD-based API which is still maintained, but it is not growing  \n",
    "  For supervised learning tasks*, the RDD API uses a `LabeledPoint` object to bundle labels with predictors.\n",
    "  \n",
    "  For unsupervised learning tasks, since there is no label, the `LabeledPoint` object is not used.  \n",
    "  Examples of unsupervised learning tasks include clustering methods like k-means.\n",
    "\n",
    "Some functionality is only available in the RDD-based API.  \n",
    "We will discuss both APIs in this course. \n",
    "\n",
    "\n",
    "(\\*) In *supervised learning* tasks, each observation has a label or ground truth indicating the correct answer.  \n",
    "Unsupervised learning tasks do NOT have this label. Most data in the wild does not have the label.\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction to Classification**\n",
    "\n",
    "Classification is a common form of supervised learning.  \n",
    "In supervised learning, the training examples include labels.  \n",
    "After training the model, the purpose of the task is to predict labels for new examples.  \n",
    "\n",
    "The data type of the $Y$ variable makes it a *classification problem*, namely $Y$ is a discrete variable.  \n",
    "Binary classification is most common. Examples include fraud (or not), default, survival, claim filing, spam.\n",
    "\n",
    "A continuous $Y$ variable results in a regression problem (next topic).\n",
    "\n",
    "Classification and regression both use the `LabeledPoint` class.  \n",
    "To remind ourselves, a `LabeledPoint` consists of a label and a features vector.  \n",
    "\n",
    "Follow this convention for labels:  \n",
    "- For binary classification, use labels $0$ and $1$  \n",
    "- For multiclass classification, use labels $0$, $1$, …, $C-1$ where $C$ is the number of classes  \n",
    "\n",
    "\n",
    "\n",
    "Spark supports several popular models for classification including:  \n",
    "- Logistic regression  \n",
    "- Naive Bayes  \n",
    "- Tree methods (e.g., decision tree, random forest)  \n",
    "- Support Vector Machines  \n",
    "\n",
    "\n",
    "\n",
    "**Logistic regression**    \n",
    "This is currently the most popular method for binary classification.  \n",
    "It is a generalized linear model which uses a linear plane to separate positive and negative examples.  \n",
    "Although the model is relatively simple, the results can be very competitive.  \n",
    "\n",
    "Below is an example of some data and a logistic curve fit to the data. Probability of Passing $Y$ is a function of Hours Studying $X$.  Notice the $Y$ variable consists of the values 0, 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logreg_img2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib includes the stochastic gradient descent (`SGD`) and `L-BFGS` algorithms for fitting the model.  \n",
    "`L-BFGS` is an approximation to Newton’s method that converges faster; it is the recommended method. \n",
    "\n",
    "From the documentation:\n",
    "\n",
    "“ `L-BFGS` version is strongly recommended since it converges faster and more accurately compared to `SGD` by approximating the inverse Hessian matrix using quasi-Newton method.”\n",
    "\n",
    "Multiclass Problems  \n",
    "The algorithm will output a multinomial logistic regression model, which contains $K−1$ binary logistic regression models regressed against the first class. Given a new data point, $K−1$ models will be run, and the class with largest probability will be chosen as the predicted class.\n",
    "\n",
    "**Logistic Regression Implementation**\n",
    "\n",
    "The names of the model fitting functions include the algorithm applied, when there is a choice.  \n",
    "For logistic regression, the following functions are supported:\n",
    "\n",
    "- `LogisticRegressionWithLBFGS`\n",
    "- `LogisticRegressionWithSGD`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODULES, CONTEXT, AND PATHING\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"mllib_classifier\") \\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression: load data/train model/predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "data = sc.textFile('sample_svm_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedData = data.map(parsePoint)\n",
    "\n",
    "# Print a record to understand the data structure\n",
    "print(parsedData.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = LogisticRegressionWithSGD.train(parsedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model on training data\n",
    "labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "print(labelsAndPreds.take(3))\n",
    "\n",
    "# Source: https://spark.apache.org/docs/latest/mllib-linear-methods.html#logistic-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes**  \n",
    "\n",
    "Naive Bayes (NB) is a relatively simple model, yet the performance can be quite good.  This has led to its popularity.  \n",
    "\n",
    "NB does multiclass classification. It is commonly used in text classification where the input features are count variables.\n",
    "\n",
    "At a high level, the count of a word on a page can adjust the probability that the page belongs to a given class.  For example, the presence of the word “tacos” will increase the probability that the page belongs to a **restaurant** relative to a **florist**.\n",
    "\n",
    "The algorithm computes the conditional probability distribution of each feature given a label, and then it applies Bayes’ theorem to compute the conditional probability distribution of a label given an observation.\n",
    "\n",
    "Naive?  \n",
    "The term “naive” comes from the simplifying assumption of independence between every pair of features. This assumption greatly simplifies the model and is often reasonable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes Implementation**  \n",
    "\n",
    "Two methods are supported:  \n",
    "\n",
    "- multinomial naive Bayes (function name: `NaiveBayes`)\n",
    "- Bernoulli naive Bayes\n",
    "\n",
    "**Parameters**  \n",
    "The model type is selected with an optional parameter “multinomial” or “bernoulli” with “multinomial” as the default.  \n",
    "\n",
    "Additive smoothing can be used by setting the parameter $λ$ (default = 1.0)  \n",
    "\n",
    "For document classification, the input feature vectors are usually sparse, and sparse vectors should be supplied as input to take advantage of sparsity. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes Example: load data/train model/predict**\n",
    "\n",
    "*Preamble*  \n",
    "\n",
    "The data used in this example is in libsvm format.  \n",
    "This has sparse format like this:  \n",
    "\n",
    "`0 1:51 2:159 3:253`\n",
    "\n",
    "`label index1:value1 index2:value2…`\n",
    "\n",
    "The data is read using:   `MLUtils.loadLibSVMFile`\n",
    "\n",
    "`MLUtils`  \n",
    "Helper methods to load, save and pre-process data used in `MLlib`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load the data file. Note this data is in sparse format.\n",
    "data = MLUtils.loadLibSVMFile(sc, 'sample_libsvm_data.txt')\n",
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data approximately into training (60%) and test (40%)\n",
    "training, test = data.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a naive Bayes model.\n",
    "model = NaiveBayes.train(training, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction and test accuracy.\n",
    "labelsAndPreds = test.map(lambda p: (p.label, model.predict(p.features)))\n",
    "accuracy = 1.0 * labelsAndPreds.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
    "print('model accuracy {}'.format(accuracy))\n",
    "\n",
    "# Source: https://spark.apache.org/docs/latest/mllib-naive-bayes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tree Methods**  \n",
    "\n",
    "Tree methods can be used for both classification and regression  \n",
    "\n",
    "Simplest method is a Decision Tree, which is intuitively appealing due to series of binary decisions (Male/Female, Age greater than 30 or not)  \n",
    "\n",
    "Can handle missing values (in many implementations), categorical data, continuous data.  \n",
    "Minimal preprocessing needed.  \n",
    "\n",
    "Feature selection is part of algorithm (best feature is used, then next best, …)  \n",
    "\n",
    "Does not require scaling  \n",
    "Handles non-linear interactions  \n",
    "Handles multiclass classification  \n",
    "\n",
    "**Decision Tree Architecture**  \n",
    "\n",
    "Tree uses binary splits on one feature at a time  \n",
    "Top of tree is the root  \n",
    "Paths or branches emanate from nodes  \n",
    "Bottom layer of nodes called the leaf nodes, which contain predictions  \n",
    "\n",
    "**Decision Tree Implementation**  \n",
    "\n",
    "`mllib.tree.DecisionTree` class  \n",
    "`trainClassifier()`\n",
    "\n",
    "Implementation partitions data by rows, allowing distributed training with millions of instances\n",
    "\n",
    "Parameters  \n",
    "Node impurity measures the homogeneity of the labels in the leaf nodes.  \n",
    "Two options are available for classification: Gini impurity and entropy.\n",
    "\n",
    "**Decision Tree Example: load data/train model/predict**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load and parse the data file\n",
    "data = MLUtils.loadLibSVMFile(sc, 'sample_libsvm_data.txt')\n",
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a DecisionTree model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "model = DecisionTree.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tree-Based Ensemble Methods**\n",
    "\n",
    "*Ensembles* combine multiple models together to produce a new model.  \n",
    "They may consist of models of the same type (e.g., all decision trees) or mixed type (e.g., decision tree + neural net + svm)  \n",
    "\n",
    "One of the fundamental results in machine learning is that multiple weak classifiers can be combined to produce a strong classifier.  \n",
    "\n",
    "Ensembles are useful in reducing overfitting, since predictions are based on several different trees  \n",
    "\n",
    "The two most popular tree-based ensemble methods are *Random Forests* and *Boosted Trees* (e.g. *Gradient-Boosted Trees*)  \n",
    "\n",
    "They are popular because they are often very competitive  \n",
    "\n",
    "The nice properties of decision trees carry over to ensembles of trees  \n",
    "\n",
    "This combining step can proceed using different methods, including:  \n",
    "\n",
    "- voting (for classification)\n",
    "- averaging (for regression) \n",
    "- running model predictions through another model (classification and regression)\n",
    "\n",
    "There are downsides to ensembles:  \n",
    "\n",
    "- Multiple models need to be trained, loaded, and maintained  \n",
    "- Model explanation is harder: no p-values like regression, several trees are feeding overall decision.  \n",
    "There are methods to provide feature importance information, such as partial dependence plots.\n",
    "\n",
    "**Random Forest**  \n",
    "Ensembles of decision trees  \n",
    "\n",
    "RFs inject two sources of randomness into modeling:  \n",
    "\n",
    "1. At each step, randomly select $p$ features out of $n$ total features for possible inclusion (random subspace method)\n",
    "2. Sample the original training set with replacement, up to the size of the original training set (bootstrapping of the training set)\n",
    "\n",
    "The number of features to randomly select $p$ is a parameter  \n",
    "The number of bootstrapped trees to grow $N$ is a parameter  \n",
    "\n",
    "Since the trees are grown independently, the training and prediction tasks are embarrassingly parallel and can be assigned to multiple workers.\n",
    "\n",
    "Classification prediction done by majority vote across trees\n",
    "\n",
    "**Random Forest Implementation**\n",
    "\n",
    "`from pyspark.mllib.tree import RandomForest`  \n",
    "\n",
    "Two most important parameters (which should be tuned using $k$-fold cross validation):  \n",
    "\n",
    "- `numTrees`: Number of trees in forest\n",
    "More trees will increase accuracy but also training time  \n",
    "\n",
    "- `maxDepth`: Maximum depth of each tree in forest\n",
    "Increasing depth can increase power of model, but will take longer to train and can overfit  \n",
    "\n",
    "Other important parameters:\n",
    "\n",
    "- `subsamplingRate`: fraction of size of original training set (default=1.0 recommended)\n",
    "\n",
    "- `featureSubsetStrategy`: specified as fraction or function of total number of features\n",
    "\n",
    "**Random Forest Example: load data/train model/predict**  \n",
    "NOTE: Very similar to Decision Tree code above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, 'sample_libsvm_data.txt')\n",
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForest model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "#  Setting featureSubsetStrategy=\"auto\" lets the algorithm choose.\n",
    "model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=1000, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient-Boosted Trees**  \n",
    "\n",
    "GBTs work by building a sequence of trees and combining their predictions at each iteration.  The trees constructed are generally *stumps* which use a single decision split.  A stump is an example of a weak learner.\n",
    "\n",
    "This is different from random forests, where each tree independently gives predictions on each training instance.\n",
    "\n",
    "\n",
    "\n",
    "A loss is specified and an optimization problem is solved whereby the objective is to minimize the loss of the model by adding weak learners using a gradient-descent-like procedure.\n",
    "\n",
    "The procedure follows a stage-wise additive model, meaning that one new weak learner is\n",
    "added at a time and existing weak learners are left unchanged.\n",
    "For the original work, see:\n",
    "\n",
    "*Friedman, Jerome H. \"Greedy function approximation: a gradient boosting machine.\" Annals of Statistics (2001): 1189–1232.*\n",
    "\n",
    "\n",
    "**Gradient-Boosted Trees Implementation**  \n",
    "\n",
    "Since the trees are built in a sequential fashion, the algorithm can not be run in parallel.  \n",
    "However, shallow trees (e.g., stumps) can be used effectively; this saves time versus random forests, which use deeper trees.\n",
    "\n",
    "The loss function in classification problems is the log loss, equal to twice the binomial negative log likelihood.\n",
    "\n",
    "Important parameters:\n",
    "- `numIterations`:  equal to the number of trees in the ensemble.  More trees means longer runtime but also better performance up to a point.\n",
    "- `learningRate`:  how quickly the model adapts on each iteration. A smaller value may help the algo have better performance, but at the cost of additional runtime. The documentation recommends NOT tuning this param.\n",
    "\n",
    "The method `runWithValidation` can help mitigate overfitting.  It takes a training RDD and a validation RDD.\n",
    "\n",
    "The training is stopped when the improvement in the validation error is not more than a certain tolerance (supplied by the `validationTol` argument in `BoostingStrategy`).\n",
    "\n",
    "**GBT Example: load data/train model/predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import GradientBoostedTrees\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, 'sample_libsvm_data.txt')\n",
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a GradientBoostedTrees model.\n",
    "model = GradientBoostedTrees.trainClassifier(trainingData, categoricalFeaturesInfo={}, numIterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY FOR YOURSELF (UNGRADED EXERCISES)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Copy the LogReg code from above into the cell below.  \n",
    "   Make the following changes:   \n",
    "   i) include an intercept  \n",
    "   ii) change iterations to 200  \n",
    "   iii) compute and output the accuracy  \n",
    "   iv) output the intercept estimate  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Copy the Naive Bayes code from above into the cell below.  \n",
    "Run the code with at least 3 different values for the smoothing parameter.  How does the parameter affect accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Copy the GradientBoostedTrees code from above into the cell below.  \n",
    "i) Run the code for different values for `numIterations`. Do you notice any relationship with the test error?  \n",
    "ii) Run the code for different values for `learningRate`. Do you notice any relationship with the test error?  \n",
    "iii) Given a slower `learningRate`, a larger number of iterations is generally needed to get the same test error.  Do you find this to be true?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "### Logistic Regression with the DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the DataFrame API, the `LabeledPoint` object is NOT used.  \n",
    "Instead, the requirement is to package all predictor columns into a single column.  \n",
    "The ML model will take the predictor column name as an input, and the target variable name as an input.\n",
    "\n",
    "There are two transformations we should discuss right away, as they are very helpful:\n",
    "\n",
    "`VectorAssembler`  \n",
    "This will package the DataFrame predictor columns into a single column.\n",
    "\n",
    "\n",
    "`StandardScaler`  \n",
    "This will scale your data, and it can be applied after the VectorAssembler step.\n",
    "\n",
    "**EXAMPLE**  \n",
    "This small dataset has target variable *high_price*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|high_price|median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|\n",
      "+----------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "|         1|       8.5552|              40.0|      880.0|         129.0|     322.0|     126.0|   37.88|  -122.23|\n",
      "|         1|       8.3252|              41.0|      880.0|         129.0|     322.0|     126.0|   37.88|  -122.23|\n",
      "+----------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the data into DF\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"ml_classifier\").getOrCreate()\n",
    "\n",
    "# Load training data\n",
    "filename = \"sample_housing_data.csv\"\n",
    "training = spark.read.csv(filename,  inferSchema=True, header = True)\n",
    "training.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `VectorAssembler` to package some variables into a feature column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+--------------+\n",
      "|high_price|median_income|housing_median_age|total_rooms|total_bedrooms|population|households|latitude|longitude|features      |\n",
      "+----------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+--------------+\n",
      "|1         |8.5552       |40.0              |880.0      |129.0         |322.0     |126.0     |37.88   |-122.23  |[8.5552,880.0]|\n",
      "|1         |8.3252       |41.0              |880.0      |129.0         |322.0     |126.0     |37.88   |-122.23  |[8.3252,880.0]|\n",
      "+----------+-------------+------------------+-----------+--------------+----------+----------+--------+---------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# inputCols take a list of column names\n",
    "# outputCol is arbitrary name of new column; generally called features\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"median_income\", \"total_rooms\"],\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "tr = assembler.transform(training)\n",
    "tr.select(\"*\").show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `StandardScaler` to scale the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------------------------+\n",
      "|high_price|features      |scaledFeatures                        |\n",
      "+----------+--------------+--------------------------------------+\n",
      "|1         |[8.5552,880.0]|[4.544281109921356,0.3640968833079785]|\n",
      "|1         |[8.3252,880.0]|[4.422111592518852,0.3640968833079785]|\n",
      "+----------+--------------+--------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(tr)\n",
    "scaledData = scalerModel.transform(tr)\n",
    "\n",
    "scaledData.select(\"high_price\",\"features\",\"scaledFeatures\").show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up and fit the model. We discuss the parameters later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.07027733010720486,0.0]\n",
      "Intercept: -0.9547053462128938\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol='high_price',\n",
    "                        featuresCol='scaledFeatures',\n",
    "                        maxIter=10, \n",
    "                        regParam=0.3, \n",
    "                        elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(scaledData)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's measure the fit. We will consider the Area under the Precision-Recall Curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------+\n",
      "|probability                             |prediction|\n",
      "+----------------------------------------+----------+\n",
      "|[0.6537005259491084,0.34629947405089156]|0.0       |\n",
      "|[0.6556415610201259,0.34435843897987406]|0.0       |\n",
      "|[0.6558421210389079,0.34415787896109207]|0.0       |\n",
      "|[0.664584376177192,0.335415623822808]   |0.0       |\n",
      "|[0.6778813168214649,0.3221186831785351] |0.0       |\n",
      "+----------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Area under PR Curve: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# compute predictions. this will append column \"prediction\" to dataframe\n",
    "lrPred = lrModel.transform(scaledData)\n",
    "lrPred.select('probability','prediction').show(5,truncate=False)\n",
    "\n",
    "# set up evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",\n",
    "                                          labelCol=\"high_price\",\n",
    "                                          metricName=\"areaUnderPR\")\n",
    "\n",
    "# pass to evaluator the DF with predictions, labels\n",
    "aupr = evaluator.evaluate(lrPred)\n",
    "\n",
    "print(\"Area under PR Curve:\", aupr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**TRY FOR YOURSELF (UNGRADED EXERCISES)**\n",
    "\n",
    "Copy all LogReg code in the cell below, modify the input columns in VectorAssembler, and refit the model.  \n",
    "Compute and print the area under the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DS5110 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
