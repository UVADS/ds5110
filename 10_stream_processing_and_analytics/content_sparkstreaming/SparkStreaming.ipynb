{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/uva_seal.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Streaming\n",
    "\n",
    "### University of Virginia\n",
    "### DS 5110: Big Data Systems\n",
    "#### Last updated: January 16, 2026\n",
    "\n",
    "---  \n",
    "\n",
    "\n",
    "### SOURCES\n",
    "\n",
    "https://spark.apache.org/docs/2.2.0/streaming-programming-guide.html\n",
    "\n",
    "https://spark.apache.org/docs/3.5.1/structured-streaming-programming-guide.html\n",
    "\n",
    "Learning Spark, Chapter 10: Spark Streaming\n",
    "\n",
    "### OBJECTIVES\n",
    "- Provide an overview of `Spark Streaming`\n",
    "- Discuss how `DStreams` work\n",
    "- Discuss the concept of window computations\n",
    "- Provide an introduction to `Structured Streaming`\n",
    "- Brief discussion of changes in Spark 4.0\n",
    "\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- StreamingContext\n",
    "- DStream\n",
    "- Window Operations\n",
    "- Receivers\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "*Spark Streaming* is a module for acting on data as soon as it arrives.   \n",
    "\n",
    "Enables scalable, high-throughput, fault-tolerant stream processing of live data streams.\n",
    "\n",
    "Data can be ingested from many sources like `Kafka`, `Flume`, `Kinesis`, or `TCP sockets`, and can be processed using complex algorithms expressed with high-level functions like `map`, `reduce`, `join` and `window.`  \n",
    "\n",
    "Finally, processed data can be pushed out to filesystems, databases, and live dashboards.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/spark_streaming_connectors.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provides high-level abstraction called `discretized stream` or `DStream`, which represents continuous stream of data.  \n",
    "\n",
    "Internally, a `DStream` is a sequence of `RDDs`.  The time interval covered by each `RDD` is the same, and is called the `batch interval`.  \n",
    "\n",
    "`DStreams` are created from input data streams like Kafka, or by applying operations on other `DStreams`\n",
    "\n",
    "**Example: Word Count of Data Received From Server Listening on TCP Socket**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "**NOTE: CODE IS FOR ILLUSTRATION PURPOSES ONLY. WE WILL RUN IN THE DEMO.**  \n",
    "\n",
    "```\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working threads\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[2]\")\\\n",
    "        .appName(\"NetworkWordCount\")\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# set up StreamingContext with batch interval of 1 second\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Create a DStream that will connect to socket = hostname:port, like localhost:8888\n",
    "# on Windows machine, can find port from CMD by issuing: netstat -a -o\n",
    "lines = ssc.socketTextStream(\"localhost\", 8888)\n",
    "\n",
    "type(lines)\n",
    "# <class 'pyspark.streaming.dstream.DStream'>\n",
    "\n",
    "# Split each line into words\n",
    "# Each line is split into words, and this stream of words goes to DStream\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# MapReduce to count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print first ten elements of each RDD generated in DStream to console\n",
    "wordCounts.pprint()\n",
    "\n",
    "# Start the computation\n",
    "# depending where you are listening, different results will follow\n",
    "ssc.start()\n",
    "\n",
    "# Wait for the computation to terminate\n",
    "ssc.awaitTermination()  \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:red'> NOTE: </span> In practice, when running on a cluster, `master` won't be hardcoded in program.  \n",
    "\n",
    "Instead, launch application with `spark-submit` and receive it there. For local testing and unit tests, `local[*]` makes sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that no computation takes place until `start()` is called on `StreamingContext`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Launching the Application**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the example, you would first install a utility called `netcat`, add it to your `PATH` variable, and issue the following at terminal.  Below the `nc` command, you can type text which should print in the Spark job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TERMINAL 1: Running Netcat on port 9999**\n",
    "```\n",
    "$ nc -lk 9999\n",
    "\n",
    "hello world\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flag `-lk` means to listen `l` for connections and to keep alive `k` for additional connections.\n",
    "\n",
    "The *port* is the endpoint for network traffic, indicating the service receiving traffic.\n",
    "\n",
    "In a second terminal, you can run the program (provided as a Spark code example).  It should print out once per second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TERMINAL 2: Running the Spark job**\n",
    "```\n",
    "$ ./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results should look something like this:\n",
    "\n",
    "```\n",
    "-------------------------------------------\n",
    "Time: 2014-10-14 15:25:21\n",
    "-------------------------------------------\n",
    "(hello,1)\n",
    "(world,1)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DStreams\n",
    "\n",
    "The `DStream` is the basic abstraction for Spark Streaming.  Each `RDD` in a `DStream` covers a time interval.  \n",
    "\n",
    "Any operation on a `DStream` maps onto each underlying RDD.\n",
    "\n",
    "The `RDD` transformations are computed by the Spark engine.  \n",
    "\n",
    "**DStream as a Sequence of RDDs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/streaming-dstream.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receivers\n",
    "\n",
    "For each input source, `Spark Streaming` launches *receivers*, which are tasks running on each executor that collect data from the input source and saves to `RDDs`. \n",
    "\n",
    "The receivers replicate the data to another executor for fault tolerance.  It is stored in executor memory, as in cached `RDDs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/exec_spark_streaming.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations on DStreams\n",
    "\n",
    "`DStreams` support many of the transformations available to `RDDs`.   Refer to [documentation](https://spark.apache.org/docs/2.2.0/streaming-programming-guide.html) for details. \n",
    "\n",
    "Transformations on `DStreams` can be characterized as *stateless* or *stateful*.\n",
    "\n",
    "- In stateless transformations, batches can be treated independently.  Previous batch data is not required.  Examples include `map` and `filter`.  \n",
    "\n",
    "\n",
    "- In stateful transformations, data from previous batches is needed to compute results on the current batch.  \n",
    "  Examples include window operations and `updateStateByKey()`, which can build up an object representing each user session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Operations\n",
    "\n",
    "`Windowed computations` allow for application of transformations over a sliding window of data.   \n",
    "In the example below, `window length=3` and `sliding interval=2`.   \n",
    "This means the operation is applied over the previous 3 `RDDs`, and it slides by 2 time units.\n",
    "\n",
    "<span style='color:red'>NOTE: </span> the parameters must be multiples of the batch interval of the `DStream`.\n",
    "\n",
    "**Windowed Computation Schematic**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/streaming-dstream-window.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wish to compute word counts for last 30 seconds of data, every 10 seconds, we can use `reduceByKey` on the `DStream` of `(word, 1)` pairs from the `Map` stage.\n",
    "\n",
    "**Compute Word Count over Last 30sec Data, every 10sec**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "```\n",
    "windowedWordCounts = pairs.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 30, 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several window operations. See [documentation](https://spark.apache.org/docs/2.2.0/streaming-programming-guide.html\n",
    ") for details.\n",
    "- `window()`\n",
    "- `countByWindow()`\n",
    "- `reduceByWindow()`\n",
    "- `reduceByKeyAndWindow()`\n",
    "- `countByValueAndWindow()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLLib Operations\n",
    "\n",
    "`SparkStreaming` is compatable with `MLlib`\n",
    "\n",
    "`MLlib` includes streaming ML algorithms which can learn from streaming data while doing scoring.\n",
    "\n",
    "Can alternatively learn a model offline and score the streaming data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching\n",
    "\n",
    "Calling `persist()` on `DStream` will persist all underlying `RDDs`.  \n",
    "This is useful when computing on the same data multiple times.  \n",
    "For window operations, this always occurs, thus the `DStreams` are automatically persisted in memory.\n",
    "\n",
    "For input streams that receive data over the network (e.g., Kafka), persisting the data will replicate on two nodes (by default) for fault-tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpointing\n",
    "\n",
    "Streaming applications must be resilient to failures external to the application (e.g., power outage, system maintenance).\n",
    "\n",
    "Information is `checkpointed` to a fault-tolerant storage system so it can easily recover from failures (e.g., `HDFS`, `S3`).  \n",
    "\n",
    "Checkpointing will save regular backups, such that the maximum information loss is equal to the information generated since the last checkpoint.\n",
    "\n",
    "Two types of information are checkpointed:\n",
    "- metadata    \n",
    "  - this backs up data on the driver node, including configs\n",
    "- data        \n",
    "  - generated RDDs   \n",
    "  - especially important if stateful transformations are used.\n",
    "  \n",
    "  setting up checkpointing is a one-liner:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "ssc.checkpoint(checkpointDirectory)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Operations\n",
    "\n",
    "Output operations bring the stream data to rest (e.g., store in a table in a database, print to screen).  They do the computation on each batch.\n",
    "\n",
    "Like actions on `RDDs`, `DStreams` are lazily evaluated.  \n",
    "When an output operation is called on a `DStream`, the computation takes place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming UI\n",
    "\n",
    "The Spark UI (typically located at *http://[driver]:4040*) contains a Streaming tab.  Here is a snapshot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/webui-streaming1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newer model that uses DataFrames for streaming\n",
    "\n",
    "Treats live data stream as an unbounded table that is being continuously appended \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/streaming_table.png' width=900>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider Word Count program using Structured Streaming\n",
    "\n",
    "Full code found [here](https://spark.apache.org/docs/3.5.1/structured-streaming-programming-guide.html). \n",
    "\n",
    "Important elements are `SparkSession.readStream` and `outputMode(\"complete\")`\n",
    "\n",
    "In *Complete Mode*, entire updated Result Table will be written to external storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram provides an illustration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./img/streaming_example.png' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is checking each second for new data from socket connection \n",
    "\n",
    "If there is new data, Spark combines previous running counts with new data to compute updated counts.\n",
    "\n",
    "**Source data is discarded**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Event Times\n",
    "\n",
    "The model can handle data by event time, rather than processing time\n",
    "\n",
    "Event time can be a column in each data row\n",
    "\n",
    "Aggregations can be done on event-time column, which handles late data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on Streaming DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common operations on DataFrames, including Spark SQL, are supported for streaming\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes in Spark 4.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Some of the changes:\n",
    "\n",
    "**1 | Lower latency**\n",
    "  - Spark 3.x: Micro-batch. latency ~100 ms to many seconds\n",
    "  - Spark 4.x: Continuous engine. latency ~5–50 ms (target). Optional, can still use micro-batch.\n",
    " \n",
    "In continuous engine, processes rows as they arrive  \n",
    "\n",
    "→ No batch intervals.  \n",
    "→ No per-batch job scheduling overhead.  \n",
    "→ Lower end-to-end latency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 | Long-running streaming jobs controlled externally without shipping code to the cluster**\n",
    "\n",
    "Spark 3.x method:\n",
    "\n",
    "- Develop code, include libraries\n",
    "- `spark-submit` it to the cluster\n",
    "- If anything changes, repackage + resubmit\n",
    "\n",
    "Spark 4.x method with Spark Connect:\n",
    "\n",
    "- Run Python script on laptop or a microservice\n",
    "\n",
    "- It sends the plan to Spark via gRPC\n",
    "\n",
    "- Spark cluster runs streaming job continuously\n",
    "\n",
    "- Can reconnect later from anywhere to manage or inspect the job\n",
    "\n",
    "What is gRPC?\n",
    "\n",
    "High-performance, open-source framework for remote procedure calls (RPCs).  \n",
    "Lets one program call functions on another program over the network as if they were local function calls.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
