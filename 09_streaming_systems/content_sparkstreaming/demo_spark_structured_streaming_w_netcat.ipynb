{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Streaming Demo\n",
    "\n",
    "#### University of Virginia\n",
    "### DS 5110: Big Data Systems\n",
    "#### Last updated: January 16, 2026\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n",
    "- [netcat](https://netcat.sourceforge.net/)\n",
    "- Spark structured streaming [documentation](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n",
    "- Spark streaming source code [example](https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/network_wordcount.py)\n",
    "- Spark **structured streaming** source code [example](https://github.com/apache/spark/blob/master/examples/src/main/python/sql/streaming/structured_network_wordcount.py)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About  \n",
    "\n",
    "This is a quick demo of word count programs of text data received from a data server listening on a TCP socket.  \n",
    "It uses the *netcat* utility as a backend.  \n",
    "On the client side, Spark Streaming is used for analytics such as word count.  \n",
    "\n",
    "We review two examples which are different. One task is to note the differences.\n",
    "\n",
    "1. Streaming word count (following the SparkStreaming notebook)\n",
    "2. Structured streaming, which uses a Spark DataFrame.  \n",
    "  The two important methods are `readStream()` and `writeStream()`  \n",
    "\n",
    "---\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Review the results below and then clear out the results to start over.\n",
    "\n",
    "Try setting up the demos and running them. For each example, you will:\n",
    "\n",
    "1) Enter text in a separate terminal running *netcat*  \n",
    "2) Run the Spark code\n",
    "\n",
    "Batches of results should print below.  \n",
    "Entering more text in *netcat* will trigger additional output below.\n",
    "\n",
    "When you are finished with each demo, stop the kernel.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background for Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backend Server\n",
    "\n",
    "For the backend server, open a terminal and run at command line:  \n",
    "\n",
    "$nc -lk 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, enter lines of text, pressing ENTER to complete each line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frontend Client\n",
    "\n",
    "For the frontend client, you will run the Spark Streaming code below\n",
    "\n",
    "Each time you enter more text in *netcat*, this will trigger a new batch of results through Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quit an experiment and run another, you can restart the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# socket parameters\n",
    "host = 'localhost'\n",
    "port = 9999\n",
    "\n",
    "# set up context\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[2]\")\\\n",
    "        .appName(\"NetworkWordCount\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "#sc = SparkContext(appName=\"PythonStreamingNetworkWordCount\")\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create text stream object\n",
    "lines = ssc.socketTextStream(host, port)\n",
    "\n",
    "# calculate word counts\n",
    "counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "              .map(lambda word: (word, 1))\\\n",
    "              .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "counts.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can shut down the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Streaming Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# socket parameters\n",
    "host = 'localhost'\n",
    "port = 9999\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"StructuredNetworkWordCount\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create DataFrame representing the stream of input lines from connection to host:port\n",
    "lines = spark\\\n",
    "    .readStream\\\n",
    "    .format('socket')\\\n",
    "    .option('host', host)\\\n",
    "    .option('port', port)\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the lines into words. `explode()` turns each array item into separate row.\n",
    "words = lines.select(\n",
    "    explode(\n",
    "        split(lines.value, ' ')\n",
    "    ).alias('word')\n",
    ")\n",
    "\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy('word').count()\n",
    "\n",
    "# Start running the query that prints the running counts to the console\n",
    "query = wordCounts\\\n",
    "    .writeStream\\\n",
    "    .outputMode('complete')\\\n",
    "    .format('console')\\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can shut down the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What differences do you notice between these examples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Streaming Word Count with Filter\n",
    "\n",
    "Provide a screening list of words to match against.  \n",
    "Include a filter to calculate a running count of the words provided in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Create your own streaming demo.\n",
    "\n",
    "Try a different streaming job below.  \n",
    "For example, you might aggregate the data differently in combination with a filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
